{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAN_v5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yNNZWXV3q6ib",
        "Z3ne3CZvrNO1",
        "GUklDOhP35BY",
        "UQgwKu-W8-o9",
        "PQTwwLpB76L_",
        "9Ic2Hjrb8tj3",
        "ptsreRts8DWK",
        "u2EK5gwh9Wh7",
        "bPOtstASFyIZ",
        "rN4Kslf4-Pnz",
        "rAOT-UFDGC-D",
        "q6oKLd4K6vOH",
        "PCdu2up4L0m5",
        "Y0UuACCKAe3c",
        "TiKJvJma7O2U",
        "8qJFkvnZBvbQ",
        "WMYHH0f5A5oj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/htanwar922/Language-Adversarial-Network/blob/master/Work/LAN_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yohubJPDla8y",
        "colab_type": "text"
      },
      "source": [
        "## Mount the Goggle Drive\n",
        "#### Change directory to 'My Drive/Colab Notebooks/Work/' ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW8PIpkLCrfs",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c5d45943-d3f5-4c7b-9800-da03fc4a5bf0"
      },
      "source": [
        "#@title mount\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd '/gdrive/My Drive/Colab Notebooks/SRIN Work'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Colab Notebooks/SRIN Work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfwhfl94l4qC",
        "colab_type": "text"
      },
      "source": [
        "## Pass command line arguments\n",
        "### To pass in Colab/Notebook, enter the arguments in sys.argv list. Also uncomment the notebook line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS4dJVSEr-wa",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ffb9dc4d-bf4f-4de4-d66b-f78fa0fb68b4"
      },
      "source": [
        "#@title options.py\n",
        "\n",
        "#!/usr/bin/env ipython\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import os, sys, logging, argparse\n",
        "from pathlib import Path\n",
        "\n",
        "__version__ = int(input(\"Enter LAN version number (to use for saving and loading purposes) : \"))\n",
        "\n",
        "if '__file__' not in locals() and '__file__' not in globals():\n",
        "    __file__ = f'/gdrive/My Drive/Colab Notebooks/SRIN Work/LAN_v{__version__}.ipynb'\n",
        "os.chdir(os.path.dirname(Path(__file__)))\n",
        "sys.argv = [__file__, '--learning_rate', '0.1', '--Q_learning_rate', '0.1', '--clipvalue', '10', '--epochs', '5', '--n_vecs', '-1', '--train_size_src', '-1', '--train_size_tgt', '-1', '--batch_size', '50000', '--vector_length', '10']#, '--no_F_bn', '--no_P_bn', '--no_Q_bn']\n",
        "sys.argv += ['--notebook', 'True']\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "\n",
        "#platform arguments\n",
        "parser.add_argument('--notebook', type=bool, default=False)\n",
        "\n",
        "# dataset arguments\n",
        "parser.add_argument('--data_path', default=None)\n",
        "parser.add_argument('--src_lang', default='en')\n",
        "parser.add_argument('--tgt_lang', default='fr')\n",
        "parser.add_argument('--train_size_src', type=int, default=None)        # use all\n",
        "parser.add_argument('--train_size_tgt', type=int, default=None)        # use all\n",
        "parser.add_argument('--num_labels', type=int, default=5+1)            # max reviews rating\n",
        "parser.add_argument('--iterate', action='store_true')                # read through iterations\n",
        "parser.add_argument('--label_dtype', default=tf.int32)\n",
        "\n",
        "# sequences and vocab arguments\n",
        "parser.add_argument('--max_seq_len', type=int, default=100)            # None for no truncate\n",
        "parser.add_argument('--unk_tok', type=str, default='<unk>')\n",
        "parser.add_argument('--bos_tok', type=str, default='<s>')\n",
        "parser.add_argument('--eos_tok', type=str, default='</s>')\n",
        "\n",
        "# training arguments\n",
        "parser.add_argument('--epochs', type=int, default=5)\n",
        "parser.add_argument('--random_seed', type=int, default=1)\n",
        "parser.add_argument('--batch_size', type=int, default=10000)\n",
        "parser.add_argument('--buffer_size', type=int, default=40000)\n",
        "parser.add_argument('--learning_rate', type=float, default=0.05)\n",
        "parser.add_argument('--Q_learning_rate', type=float, default=0.05)\n",
        "\n",
        "# bwe arguments\n",
        "parser.add_argument('--emb_filename', default='')\n",
        "parser.add_argument('--n_vecs', type=int, default=-1)\n",
        "parser.add_argument('--random_emb', action='store_true')\n",
        "parser.add_argument('--fix_unk', action='store_true')                # use a fixed <unk> token for all words without pretrained embeddings when building vocab\n",
        "parser.add_argument('--emb_size', type=int, default=300)\n",
        "parser.add_argument('--pre_trained_src_emb_file', type=str, default='bwe/vectors/wiki.multi.en.vec')\n",
        "parser.add_argument('--pre_trained_tgt_emb_file', type=str, default='bwe/vectors/wiki.multi.fr.vec')\n",
        "\n",
        "# Feature Extractor\n",
        "parser.add_argument('--model', default='lstm')                        # dan or lstm or cnn\n",
        "parser.add_argument('--fix_emb', action='store_true')\n",
        "parser.add_argument('--vector_length', type=int, default=1)\n",
        "# for LSTM model\n",
        "parser.add_argument('--attn', default='dot')                        # attention mechanism (for LSTM): avg, last, dot\n",
        "parser.add_argument('--bidir_rnn', dest='bidir_rnn', action='store_true', default=True)        # bi-directional LSTM\n",
        "parser.add_argument('--sum_pooling/', dest='avg_pooling', action='store_false')\n",
        "parser.add_argument('--avg_pooling/', dest='avg_pooling', action='store_true')\n",
        "# for CNN model\n",
        "parser.add_argument('--kernel_num', type=int, default=400)\n",
        "parser.add_argument('--kernel_sizes', type=int, nargs='+', default=[3,4,5])\n",
        "\n",
        "# for layers and all models\n",
        "parser.add_argument('--F_layers', type=int, default=1)\n",
        "parser.add_argument('--P_layers', type=int, default=1)\n",
        "parser.add_argument('--Q_layers', type=int, default=1)\n",
        "\n",
        "parser.add_argument('--q_critic', type=int, default=5)    # Q iterations\n",
        "parser.add_argument('--_lambda', type=float, default=0.1)\n",
        "\n",
        "parser.add_argument('--F_bn/', dest='F_bn', action='store_true')\n",
        "parser.add_argument('--no_F_bn/', dest='F_bn', action='store_false')\n",
        "parser.add_argument('--P_bn/', dest='P_bn', action='store_true', default=True)\n",
        "parser.add_argument('--no_P_bn/', dest='P_bn', action='store_false')\n",
        "parser.add_argument('--Q_bn/', dest='Q_bn', action='store_true', default=True)\n",
        "parser.add_argument('--no_Q_bn/', dest='Q_bn', action='store_false')\n",
        "\n",
        "parser.add_argument('--hidden_size', type=int, default=900)\n",
        "parser.add_argument('--dropout', type=float, default=0)\n",
        "parser.add_argument('--activation', type=str, default='linear')\n",
        "\n",
        "parser.add_argument('--clip_Q', type=bool, default=False)\n",
        "parser.add_argument('--clipvalue', type=float, default=0.01)\n",
        "parser.add_argument('--clip_lim_FP', type=float, default=None)\n",
        "\n",
        "# general arguments\n",
        "parser.add_argument('--device', type=str, default='cuda')\n",
        "parser.add_argument('--debug/', dest='debug', action='store_true')\n",
        "parser.add_argument('--__version__', type=int, default=__version__)\n",
        "\n",
        "# crash arguments\n",
        "parser.add_argument('--start_fresh/', dest='start_fresh', action='store_false', default=False)\n",
        "parser.add_argument('--saved_models', default=Path(f'./saved_models/lan_v{__version__}'))\n",
        "parser.add_argument('--logs', default=Path(f'./saved_models/lan_v{__version__}/logs.txt'))\n",
        "parser.add_argument('--crash_logs', default=Path(f'./saved_models/lan_v{__version__}/crash_logs.txt'))\n",
        "parser.add_argument('--ckpt_prefix', default=Path(f'./saved_models/lan_v{__version__}/ckpts'))\n",
        "parser.add_argument('--last_ckpt', default=0)\n",
        "\n",
        "opt = parser.parse_args()\n",
        "\n",
        "opt.saved_models = Path(opt.saved_models)\n",
        "opt.logs = Path(opt.logs)\n",
        "opt.crash_logs = Path(opt.crash_logs)\n",
        "opt.ckpt_prefix = Path(opt.ckpt_prefix)\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'): opt.device = 'CPU'\n",
        "\n",
        "import errno\n",
        "logging.basicConfig(stream=sys.stderr, level=logging.DEBUG if opt.debug else logging.INFO)\n",
        "log = logging.getLogger(__name__)\n",
        "if not os.path.isdir(os.path.dirname(opt.logs)): os.mkdir(os.path.dirname(opt.logs))\n",
        "open(opt.logs, 'w').close()\n",
        "if not os.path.exists(os.path.dirname(opt.logs)):\n",
        "    try: os.makedirs(os.path.dirname(opt.logs))\n",
        "    except OSError as exc:\n",
        "        if exc.errno != errno.EEXIST: raise # Guard against race condition\n",
        "with open(opt.logs, \"w\") as f: pass\n",
        "fh = logging.FileHandler(opt.logs)  #Path(opt.saved_models) / 'log.txt'\n",
        "log.addHandler(fh)\n",
        "\n",
        "if not os.path.exists(os.path.dirname(opt.crash_logs)):\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(opt.crash_logs))\n",
        "        opt.start_fresh = True\n",
        "    except OSError as exc:\n",
        "        if exc.errno != errno.EEXIST: raise # Guard against race condition\n",
        "\n",
        "#opt.start_fresh = True\n",
        "if opt.start_fresh or not os.path.isfile(opt.crash_logs):\n",
        "    with open(opt.crash_logs, 'w') as foo:\n",
        "        foo.write('0')\n",
        "else:\n",
        "    with open(opt.crash_logs, 'r') as foo:\n",
        "        opt.last_ckpt = foo.read()\n",
        "        try:\n",
        "            opt.last_ckpt = int(opt.last_ckpt)\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"src_embeddings: \", opt.pre_trained_src_emb_file)\n",
        "    print(\"tgt_embeddings: \", opt.pre_trained_tgt_emb_file)\n",
        "    print(\"debugging: \", opt.debug)\n",
        "    print(f'Starting fresh : {opt.start_fresh}')\n",
        "    print(f'Save models at : {opt.saved_models}')\n",
        "    print(f'Logging at : {opt.logs}')\n",
        "    print(f'Crash logging at : {opt.crash_logs}')\n",
        "    print(f'Checkpoints prefixed in : {opt.ckpt_prefix}')\n",
        "    log.info(' Start...')\n",
        "    log.info(f' LAN Version {opt.__version__}')\n",
        "    print(f'Resuming from checkpoint : {opt.last_ckpt}')\n",
        "    opt.current_ckpt = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter LAN version number (to use for saving and loading purposes) : 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__: Start...\n",
            "INFO:__main__: LAN Version 5\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src_embeddings:  bwe/vectors/wiki.multi.en.vec\n",
            "tgt_embeddings:  bwe/vectors/wiki.multi.fr.vec\n",
            "debugging:  False\n",
            "Starting fresh : False\n",
            "Save models at : saved_models/lan_v5\n",
            "Logging at : saved_models/lan_v5/logs.txt\n",
            "Crash logging at : saved_models/lan_v5/crash_logs.txt\n",
            "Checkpoints prefixed in : saved_models/lan_v5/ckpts\n",
            "Resuming from checkpoint : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgY3G_4FmXiS",
        "colab_type": "text"
      },
      "source": [
        "## Class definition to load pre-trained word embeddings of SRC and TGT languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnLNtzFHf4Qu",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title vocab.py\n",
        "\n",
        "#!/usr/bin/env ipython\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os, io, sys, logging\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm, trange\n",
        "os.chdir(os.path.dirname(__file__))\n",
        "\n",
        "#from options import *\n",
        "\n",
        "class Vocab:\n",
        "    \"\"\"\n",
        "attributes: (self)\n",
        "        vocab_size\n",
        "        emb_size\n",
        "        embeddings \n",
        "        w2vvocab = {word : idx}\n",
        "        v2wvocab = [idx => emb_vector]\n",
        "        pt_w2vvocab = {word : idx}\n",
        "        pt_v2wvocab = [idx => emb_vector]\n",
        "        unk_tok\n",
        "        unk_idx\n",
        "        eos_tok\n",
        "        eos_idx\n",
        "\n",
        "methods: (self)\n",
        "        __init__(self, pre_train_infile)\n",
        "        base_form(word)\n",
        "        new_rand_emb(self)\n",
        "        init_embed_layer(self)\n",
        "        add_word(self, word)\n",
        "        clear_pretrained_vectors(self)\n",
        "        lookup(self, word)\n",
        "        get_word(self, i)\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, pre_train_infile = None, vecs = opt.n_vecs):\n",
        "        \"\"\"\n",
        "        load pre-trained words - embedding vectors in pt_***vocabs and initialise ***vocabs\n",
        "\n",
        "        \"\"\"\n",
        "        self.vocab_size = 0\n",
        "        self.emb_size = opt.emb_size\n",
        "        self.embeddings = []\n",
        "        self.w2vvocab = {}\n",
        "        self.v2wvocab = []\n",
        "        \n",
        "        self.pt_v2wvocab = []\n",
        "        self.pt_w2vvocab = {}\n",
        "        self.cnt = 0\n",
        "        \n",
        "        # add <unk>\n",
        "        self.unk_tok = opt.unk_tok\n",
        "        self.add_word(self.unk_tok)\n",
        "        opt.unk_idx = self.unk_idx = self.w2vvocab[self.unk_tok]\n",
        "        self.embeddings[self.unk_idx][:] = 0\n",
        "        # add BOS token\n",
        "        self.bos_tok = opt.bos_tok\n",
        "        self.add_word(self.bos_tok)\n",
        "        opt.bos_idx = self.bos_idx = self.unk_idx #self.w2vvocab[self.bos_tok]\n",
        "        #self.embeddings[self.bos_idx][:] = 0\n",
        "        # add EOS token\n",
        "        self.eos_tok = opt.eos_tok\n",
        "        self.add_word(self.eos_tok)\n",
        "        opt.eos_idx = self.eos_idx = self.unk_idx #self.w2vvocab[self.eos_tok]\n",
        "        #self.embeddings[self.eos_idx][:] = 1    # 0\n",
        "        log.info(\"vocab initializing...done.\")\n",
        "        # add pre trained embeddings\n",
        "        self.add_pre_trained_emb(pre_train_infile, vecs)\n",
        "\n",
        "    def add_pre_trained_emb(self, pre_train_infile = None, vecs = opt.n_vecs):\n",
        "        # load pretrained embedings\n",
        "        if(pre_train_infile is None): raise Exception('file not specified...')\n",
        "        if(os.path.isfile(pre_train_infile)):\n",
        "            log.info('reading pre-trained embeddings from ' + pre_train_infile + '...')\n",
        "            with io.open(Path(pre_train_infile), 'r', encoding='utf-8') as infile:\n",
        "                first_line = infile.readline().split()\n",
        "                assert len(first_line) == 2\n",
        "                n_vecs, emb_dim = map(int, first_line)    # first line has total number of vectors and embedding dimensions\n",
        "                assert emb_dim == self.emb_size\n",
        "                self.emb_size = emb_dim\n",
        "                if vecs is not None and vecs > 0: n_vecs = min(n_vecs, vecs)\n",
        "                if not hasattr(self, 'pretrained'):    self.pretrained = np.empty(shape=(n_vecs, emb_dim), dtype=np.float)\n",
        "                else: self.pretrained = np.append(self.pretrained, np.empty(shape=(n_vecs, emb_dim), dtype=np.float), axis=0)\n",
        "                for _ in trange(n_vecs):\n",
        "                    line = infile.readline()\n",
        "                    if not line: break\n",
        "                    parts = line.rstrip().split(' ')\n",
        "                    word = parts[0]\n",
        "                    #if word in self.pt_v2wvocab: continue        # no need to check if assumed no repetition mistake\n",
        "                    # add to vocabs\n",
        "                    self.pt_v2wvocab.append(word)\n",
        "                    self.pt_w2vvocab[word] = self.cnt\n",
        "                    vector = [float(x) for x in parts[1:]]\n",
        "                    self.pretrained[self.cnt] = vector\n",
        "                    self.cnt += 1\n",
        "            log.info(\"embedding vectors imported...\")\n",
        "        else:\n",
        "            raise FileNotFoundError(log.info(\"pre_train_file \", Path(pre_train_infile), \" does not exist...\"))\n",
        "\n",
        "    def base_form(self, word):\n",
        "        \"\"\"\n",
        "        return stripped and lowercased word\n",
        "        \"\"\"\n",
        "        return word.strip().lower()\n",
        "\n",
        "\n",
        "    def new_rand_emb(self):\n",
        "        \"\"\"\n",
        "        return a normal random emb_vector\n",
        "        \"\"\"\n",
        "        vec = np.random.normal(-1, 1, size=self.emb_size)\n",
        "        vec /= sum(x*x for x in vec) ** .5\n",
        "        return vec\n",
        "\n",
        "\n",
        "    def add_word(self, word, vec=None):\n",
        "        \"\"\"\n",
        "        add new word to the ***vocab. \\nUse this to only add new words or used words from pt_***vocabs to ***vocabs.\n",
        "        \"\"\"\n",
        "        word = self.base_form(word=word)\n",
        "        if word not in self.w2vvocab:\n",
        "            if not opt.random_emb and hasattr(self, 'pt_w2vvocab'):\n",
        "                if opt.fix_unk and word not in self.pt_w2vvocab:\n",
        "                    # use fixed unk token, do not update vocab\n",
        "                    return\n",
        "                if word in self.pt_w2vvocab:\n",
        "                    vector = self.pretrained[self.pt_w2vvocab[word]].copy()\n",
        "                else:\n",
        "                    vector = self.new_rand_emb() if vec is None else vec\n",
        "            else:\n",
        "                vector = self.new_rand_emb() if vec is None else vec\n",
        "            self.v2wvocab.append(word)\n",
        "            self.w2vvocab[word] = self.vocab_size\n",
        "            self.embeddings.append(vector)\n",
        "            self.vocab_size += 1\n",
        "        return self.w2vvocab[word]\n",
        "\n",
        "\n",
        "    def lookup(self, word):\n",
        "        \"\"\"\n",
        "        return value of word (word_idx) from w2vvocab\n",
        "        \"\"\"\n",
        "        word = Vocab.base_form(word)\n",
        "        if word in self.w2vvocab:\n",
        "            return self.w2vvocab[word]\n",
        "        return self.unk_idx\n",
        "\n",
        "\n",
        "    def get_word(self, i):\n",
        "        \"\"\"\n",
        "        return emb_vector at index i\n",
        "        \"\"\"\n",
        "        return self.v2wvocab[i]\n",
        "    \n",
        "\n",
        "    def hash_fit_on_text(self, line):\n",
        "        \"\"\"\n",
        "        fit on text (sentence) with tf.keras.preprocessing.text.hashing_trick\\n(NOT TESTED)\n",
        "        \"\"\"\n",
        "        return preprocessing.text.hashing_trick(line, n=self.vocab_size, hash_function=self.lookup, filters='')\n",
        "\n",
        "\n",
        "    def text_to_sequence(self, line, update_vocab=True):\n",
        "        \"\"\"\n",
        "        convert text (line) to sequence\n",
        "        \"\"\"\n",
        "        return [self.add_word(w) for w in line.strip().split()] if update_vocab else\\\n",
        "            [self.lookup(w) for w in line.strip().split()]\n",
        "\n",
        "\n",
        "    def text_list_to_sequence(self, text_list, update_vocab=True):\n",
        "        \"\"\"\n",
        "        convert text (word list) to sequence\n",
        "        \"\"\"\n",
        "        return [self.add_word(w) for w in text_list] if update_vocab else\\\n",
        "            [self.lookup(w) for w in text_list]\n",
        "\n",
        "\n",
        "    def fit_on_text(self, line_list):    # tf.keras.preprocessing.text.hashing_trick\n",
        "        \"\"\"\n",
        "        add new text (sentence) to the vocabularies\n",
        "        \"\"\"\n",
        "        return [[self.add_word(w) for w in line.strip().split()] for line in line_list]\n",
        "\n",
        "\n",
        "    def fit_on_text_list(self, texts_list):    # tf.keras.preprocessing.text.hashing_trick\n",
        "        \"\"\"\n",
        "        add new text (sentence) to the vocabularies\n",
        "        \"\"\"\n",
        "        return [[self.add_word(w) for w in line] for line in texts_list]\n",
        "\n",
        "\n",
        "    def pad_text_list(self, text_list, max_len=opt.max_seq_len, pad='pre', truncate='post', add_eos_tok=False):\n",
        "        \"\"\"\n",
        "        pad single text (words) list\n",
        "        \"\"\"\n",
        "        if add_eos_tok: max_len -= 1\n",
        "        text_list = text_list[:max_len]\n",
        "        text_list = ['<s>' for _ in range(max_len - len(text_list))] + text_list\n",
        "        if add_eos_tok: text_list += ['</s>']\n",
        "        return text_list\n",
        "\n",
        "\n",
        "    def pad_sequences(self, dataset, max_len=opt.max_seq_len, pad='pre', truncate='post', add_eos=False):\n",
        "        \"\"\"\n",
        "        pad list of sequences (sequence : list of int) with keras.preprocessing.sequence.pad_sequences\n",
        "        \"\"\"\n",
        "        if add_eos: max_len -= 1\n",
        "        seq_list, lengths, stars = zip(*dataset)\n",
        "        seq_list = self.fit_on_text_list(seq_list)\n",
        "        seq_list = preprocessing.sequence.pad_sequences(tqdm(seq_list), maxlen=max_len, truncating='post', value=opt.bos_idx)\n",
        "        if add_eos: seq_list = preprocessing.sequence.pad_sequences(tqdm(seq_list), maxlen=max_len+1, padding='post', value=opt.eos_idx)\n",
        "        return tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(seq_list, name='seq'), tf.convert_to_tensor(lengths, name='len'), tf.convert_to_tensor(stars, name='label')))\n",
        "\n",
        "    def clear_pretrained_vectors(self):\n",
        "        \"\"\"\n",
        "        clear the pretrained vectors and pt_***vocab\n",
        "        \"\"\"\n",
        "        if hasattr(self, 'pretrained'): del self.pretrained\n",
        "        if hasattr(self, 'pt_w2vvocab'): del self.pt_w2vvocab\n",
        "        if hasattr(self, 'pt_v2wvocab'): del self.pt_v2wvocab\n",
        "    \n",
        "\n",
        "    def init_embed_layer(self, clear_pt=True):\n",
        "        \"\"\"\n",
        "        clear pretrained vectors and return an embedding layer initialized with self.embeddings\n",
        "        \"\"\"\n",
        "        if clear_pt: self.clear_pretrained_vectors()\n",
        "        emb_layer = layers.Embedding(input_dim=self.vocab_size, output_dim=self.emb_size, input_length=opt.max_seq_len, name='vocab_embedding')\n",
        "        emb_layer.build(input_shape=(None, self.vocab_size, self.emb_size))\n",
        "        emb_layer.set_weights(np.array([self.embeddings], dtype=float))\n",
        "        emb_layer.trainable = False\n",
        "        assert emb_layer.weights[0].shape[0] == self.vocab_size, \"layer weights len not equal to vocab size in layer \" + emb_layer.name\n",
        "        return emb_layer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\" and not opt.notebook:\n",
        "    \"\"\"\n",
        "    run as main\n",
        "    \"\"\"\n",
        "    print(opt.pre_trained_src_emb_file)\n",
        "    vocab = Vocab(opt.pre_trained_src_emb_file, vecs=5000)\n",
        "    vocab.add_pre_trained_emb(opt.pre_trained_tgt_emb_file, vecs=5000)\n",
        "    vocab.add_word('the')\n",
        "    vocab.add_word('of')\n",
        "    vocab.add_word('this')\n",
        "    emb_layer = vocab.init_embed_layer(clear_pt=False)\n",
        "    print(emb_layer.variables)\n",
        "\n",
        "# imp link https://www.tensorflow.org/tfx/tutorials/transform/census , https://www.tensorflow.org/api_docs/python/tf/numpy_function , https://www.tensorflow.org/api_docs/python/tf/py_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf301oYmmnOx",
        "colab_type": "text"
      },
      "source": [
        "## Some useful functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKHVCROGjkdW",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title utils.py\n",
        "\n",
        "#!/usr/bin/env ipython\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, preprocessing\n",
        "\n",
        "import pdb\n",
        "import numpy as np\n",
        "\n",
        "#from options import *\n",
        "\n",
        "DEBUG = lambda x: print(x)\n",
        "\n",
        "def argmax32(arr, axis=-1, dtype=opt.label_dtype):\n",
        "    return tf.cast(np.argmax(arr, axis=-1), dtype=dtype)\n",
        "\n",
        "def get_lines(infile, encoding='utf-8'):\n",
        "    if os.sep != '\\\\': return int(subprocess.Popen(f\"wc -l \\\"{str(Path(infile))}\\\"\", shell=True, stdout=subprocess.PIPE).stdout.read().split()[0])\n",
        "    with io.open(Path(infile), encoding=encoding) as foo:\n",
        "        lines = sum(1 for line in foo)  #os.path.getsize(infile)\n",
        "    return lines\n",
        "\n",
        "def reached_ckpt(ckpt_no):\n",
        "    opt.current_ckpt = ckpt_no\n",
        "\n",
        "def load_models(current_ckpt=None, __version__=opt.__version__):\n",
        "    LAN = models.load_model(Path(opt.ckpt_prefix) / f'ckpt_{current_ckpt}')\n",
        "    print(LAN.layers, '\\n')\n",
        "\n",
        "    _, E, _, A, F, P, Q = LAN.layers\n",
        "    print(E, A, F, P, Q, '\\n')\n",
        "    print(F.layers, P.layers, Q.layers, '\\n')\n",
        "\n",
        "    print(LAN.inputs)\n",
        "    shape = (opt.max_seq_len,)\n",
        "    inputs, lengths = LAN.inputs\n",
        "\n",
        "    embeddings = E(inputs)\n",
        "    outputs_EA = A(embeddings, lengths)\n",
        "    EA = keras.Model(inputs=[inputs, lengths], outputs=outputs_EA)\n",
        "\n",
        "    outputs_EAF = F(outputs_EA)\n",
        "    EAF = keras.Model(inputs=[inputs, lengths], outputs=outputs_EAF, name=\"FeatureExtractor_AE\")\n",
        "\n",
        "    outputs_EAFP = P(outputs_EAF)\n",
        "    EAFP = keras.Model(inputs=[inputs, lengths], outputs=outputs_EAFP, name=\"SemanticClassifier_FAE\")\n",
        "\n",
        "    outputs_EAFQ = Q(outputs_EAF)\n",
        "    EAFQ = keras.Model(inputs=[inputs, lengths], outputs=outputs_EAFQ, name=\"LanguageDetector_FAE\")\n",
        "\n",
        "    verified = verify_models(EA, EAF, EAFP, EAFQ)\n",
        "\n",
        "    return (E, A, F, P, Q, EA, EAF, EAFP, EAFQ, LAN)\n",
        "\n",
        "def verify_models(EA, EAF, EAFP, EAFQ):\n",
        "    print(EA.layers, '\\n')\n",
        "    print(EAF.layers, '\\n')\n",
        "    print(EAFP.layers, '\\n')\n",
        "    print(EAFQ.layers, '\\n')\n",
        "    return int(input(\"Enter 0 if verified OK : \"))\n",
        "\n",
        "def save_models(current_ckpt, __version__=opt.__version__):\n",
        "    LAN.save(Path(opt.ckpt_prefix) / f'ckpt_{current_ckpt}')\n",
        "    #LAN.save_weights(Path(opt.ckpt_prefix) / f'saved_weights')\n",
        "\n",
        "if __name__ == \"__main__\" and not opt.notebook:\n",
        "    print(get_lines(\"bwe/vectors/wiki.multi.en.vec\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIY8q09Wmsvv",
        "colab_type": "text"
      },
      "source": [
        "## Class to load Amazon Reviews data in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VdgqJq_iCsg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title data.py\n",
        "\n",
        "#!/usr/bin/env ipython\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import sys, os, io, json, subprocess\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm, trange\n",
        "os.chdir(os.path.dirname(__file__))\n",
        "\n",
        "#from options import opt\n",
        "#from vocab import *\n",
        "\n",
        "label_dtype = opt.label_dtype\n",
        "\n",
        "def decode_json(infile, lines=None, reviews_data=None, max_seq_len=None):\n",
        "    assert os.path.isfile(Path(infile)), str(os.getcwd() / infile) + \" doesn't exist, extract_data first\"\n",
        "    if lines is None or lines < 0: lines = get_lines(infile)\n",
        "    log.info(f'Reading {lines} lines from {infile}')\n",
        "    with io.open(Path(infile), 'r', encoding='utf-8') as infile:\n",
        "        if reviews_data == 'Amazon reviews':\n",
        "            ret = []\n",
        "            max_stars = 0\n",
        "            for _ in trange(lines): # line in infile\n",
        "                dic = json.loads(infile.readline())\n",
        "                line = str(dic[\"review_title\"] + dic[\"review_body\"]).strip().split()[:max_seq_len]\n",
        "                ret += [[line, len(line), tf.cast(int(dic['stars']), dtype=opt.label_dtype)]]\n",
        "                max_stars = max(max_stars, int(dic['stars']))\n",
        "            return ret, max_stars\n",
        "        return [json.loads(line) for line in tqdm(infile.read().strip().split('\\n')[:lines])]\n",
        "\n",
        "\n",
        "def decode_json_iterate(infile, lines=None, reviews_data=None, max_seq_len=None):\n",
        "    assert os.path.isfile(Path(infile)), str(os.getcwd() / infile) + \" doesn't exist, extract_data first\"\n",
        "    with tqdm(total=get_lines(infile) if not lines else lines) as pbar:\n",
        "        with io.open(Path(infile), 'r', encoding='utf-8') as infile:\n",
        "            if lines is not None and lines > 0: z = zip(trange(1, lines+1), infile)\n",
        "            else: z = enumerate(infile)\n",
        "            for num, line in z:\n",
        "                if not line or lines is not None and num > lines: break;\n",
        "                dic = json.loads(line)\n",
        "                pbar.update(len(line) if not lines else 1)\n",
        "                yield dic if reviews_data != 'Amazon reviews' \\\n",
        "                    else [str(dic[\"review_title\"] + dic[\"review_body\"]).strip().split()[:max_seq_len], len(str(dic[\"review_title\"] + dic[\"review_body\"]).strip().split()[:max_seq_len]), tf.cast(int(dic[\"stars\"]), dtype=opt.label_dtype)]\n",
        "\n",
        "\n",
        "class AmazonReviews:\n",
        "    \"\"\"\n",
        "    get Amazon reviews data from the extracted data => review title + review body + eos_tok : stars\n",
        "parameters:\n",
        "    path : str => path to 'Amazon reviews' directory with '/' as separator\n",
        "    eos_tok : str\n",
        "    max_seq_len : int\n",
        "    \"\"\"\n",
        "    def __init__(self, path:str=None, eos_tok=opt.eos_tok, max_seq_len=opt.max_seq_len, star_rating=5):\n",
        "        super(AmazonReviews, self).__init__()\n",
        "        self.path = Path('Amazon reviews') if not path else Path(path)\n",
        "        self.dats = {}\n",
        "        self.dats['train'] = self.path / 'train'\n",
        "        self.dats['dev'] = self.path / 'dev'\n",
        "        self.dats['test'] = self.path / 'test'\n",
        "        self.eos_tok = eos_tok\n",
        "        self.max_seq_len = max_seq_len\n",
        "        opt.labels = self.star_rating = star_rating\n",
        "\n",
        "\n",
        "    def load_data(self, lang, dat, lines=-1):\n",
        "        \"\"\"\n",
        "        load all data in one go\n",
        "parametrs:\n",
        "        lang : str => de, en, es, fr, ja, zh\n",
        "        dat : str => train, dev, test\n",
        "        lines : int\n",
        "return:\n",
        "        tuple of\n",
        "            dataset of reviews (split) and their star ratings\n",
        "            max_seq_length\n",
        "        \"\"\"\n",
        "        infile = self.dats[dat] / str('dataset_' + lang + '_' + dat + '.json')\n",
        "        data, self.star_rating = decode_json(infile, lines=lines, reviews_data='Amazon reviews', max_seq_len=self.max_seq_len)\n",
        "        return data\n",
        "\n",
        "\n",
        "    def load_data_generator(self, lang, dat, lines=-1):\n",
        "        \"\"\"\n",
        "        iterate over the data file line by line (for less RAM devices like this one - not completely implemented)\n",
        "parametrs:\n",
        "        lang : str => de, en, es, fr, ja, zh\n",
        "        dat : str => train, dev, test\n",
        "        lines : int\n",
        "yield:\n",
        "        generator witch generates a list of one review and its star rating at a time\n",
        "        \"\"\"\n",
        "        infile = self.dats[dat] / str('dataset_' + lang + '_' + dat + '.json')\n",
        "        return decode_json_iterate(infile, lines=lines, reviews_data='Amazon reviews', max_seq_len=self.max_seq_len)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\" and not opt.notebook:\n",
        "    infile = 'Amazon reviews/test/dataset_en_test.json'\n",
        "    #assert os.path.isfile(infile), str(os.getcwd() / infile) + \" doesn't exist\"\n",
        "    #for x in decode_json_iterate(infile, 30): print(x)\n",
        "    #print(decode_json(infile, 30))\n",
        "    #vocab = Vocab(opt.pre_trained_src_emb_file)\n",
        "    rev = AmazonReviews()\n",
        "    data = rev.load_data(lang='en', dat='train')\n",
        "    data = vocab.pad_sequences(data)\n",
        "    print('\\n', data)\n",
        "    for dat in data.take(3):\n",
        "        print(dat)\n",
        "    #sequence_input = keras.Input(input_shape=(opt.max_seq_len,), dtype='int32')(np.array([x for x, y in data.as_numpy_iterator()], dtype='int32'))\n",
        "    #emb_layer = vocab.init_embed_layer()\n",
        "    #emb_layer(sequence_input)\n",
        "    #print(emb_layer(tf.convert_to_tensor([x for x, y in data.as_numpy_iterator()], dtype='int32')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X0ZjV1vm3QL",
        "colab_type": "text"
      },
      "source": [
        "## Load data in the notebook\n",
        "- ### Pre-trained embeddings in vocab\n",
        "- ### Amazon reviews data in\n",
        "> - train_src, dev_src, test_src\n",
        "> - train_tgt, dev_tgt, test_tgt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxWU46Jbkyew",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "b3fe25b7-7b6d-412a-c88d-af9dce4b36ec"
      },
      "source": [
        "#@title train_data.py\n",
        "\n",
        "#!/usr/bin/env ipython\n",
        "\n",
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as functional\n",
        "#import torch.optim as optim\n",
        "#from torch.utils.data import DataLoader\n",
        "#from torchnet.meter import ConfusionMeter\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers, losses\n",
        "import json\n",
        "\n",
        "import os, random, sys, logging, argparse\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "os.chdir(os.path.dirname(__file__))\n",
        "\n",
        "#from options import *\n",
        "#from data import *\n",
        "#from vocab import *\n",
        "#from utils import *\n",
        "#from models import *\n",
        "\n",
        "#tf.logging.set_verbosity(tf.logging.INFO)\n",
        "#tf.logging.set_verbosity(True)\n",
        "\n",
        "#random.seed(opt.random_seed)\n",
        "#torch.manual_seed(opt.random_seed)\n",
        "\n",
        "# output options\n",
        "log.info('Training ADAN with options:')\n",
        "log.info(opt)\n",
        "\n",
        "def get_train_data(opt):\n",
        "    #opt.n_vecs = 20000; opt.train_size_src = -1; opt.train_size_tgt = -1\n",
        "    # vocab\n",
        "    log.info(f'Loading Embeddings...')\n",
        "    vocab = Vocab(opt.pre_trained_src_emb_file, opt.n_vecs)\n",
        "    vocab.add_pre_trained_emb(opt.pre_trained_tgt_emb_file, opt.n_vecs)\n",
        "    log.info(f'Done.')\n",
        "\n",
        "    # datasets\n",
        "    length = {}\n",
        "\n",
        "    # src_lang datasets\n",
        "    log.info(f'Loading src datasets...')\n",
        "    reviews_src_obj = AmazonReviews(path=opt.data_path, max_seq_len=opt.max_seq_len)\n",
        "    train_src = reviews_src_obj.load_data(lang=opt.src_lang, dat='train', lines=opt.train_size_src); length['train_src'] = len(train_src)\n",
        "    dev_src = reviews_src_obj.load_data(lang=opt.src_lang, dat='dev', lines=-1); length['dev_src'] = len(dev_src)\n",
        "    test_src = reviews_src_obj.load_data(lang=opt.src_lang, dat='test', lines=-1); length['test_src'] = len(test_src)\n",
        "    log.info('Done loading src datasets.')\n",
        "\n",
        "    # tgt_lang datasets\n",
        "    log.info(f'Loading tgt datasets...')\n",
        "    reviews_tgt_obj = AmazonReviews(path=opt.data_path, max_seq_len=opt.max_seq_len)\n",
        "    train_tgt = reviews_tgt_obj.load_data(lang=opt.tgt_lang, dat='train', lines=opt.train_size_tgt); length['train_tgt'] = len(train_tgt)\n",
        "    dev_tgt = reviews_tgt_obj.load_data(lang=opt.tgt_lang, dat='dev', lines=-1); length['dev_tgt'] = len(dev_tgt)\n",
        "    test_tgt = reviews_tgt_obj.load_data(lang=opt.tgt_lang, dat='test', lines=-1); length['test_tgt'] = len(test_tgt)\n",
        "    \n",
        "    log.info('Done loading tgt datasets.')\n",
        "\n",
        "    #opt.num_labels = max(reviews_src_obj.star_rating, reviews_tgt_obj.star_rating)\n",
        "    if opt.max_seq_len < 0 or not opt.max_seq_len:\n",
        "        maxlen_src, maxlen_tgt = max(list(len(x) for x in train_src)), max(list(len(x) for x in train_tgt))\n",
        "        opt.max_seq_len = max(maxlen_src, maxlen_tgt)\n",
        "    del reviews_src_obj, reviews_tgt_obj\n",
        "\n",
        "    # pad src datasets (-> Dataset)\n",
        "    log.info('Padding src datasets...')\n",
        "    train_src = vocab.pad_sequences(train_src, max_len=opt.max_seq_len)\n",
        "    dev_src = vocab.pad_sequences(dev_src, max_len=opt.max_seq_len)\n",
        "    test_src = vocab.pad_sequences(test_src, max_len=opt.max_seq_len)\n",
        "    log.info('Done padding src datasets...')\n",
        "\n",
        "    # pad tgt datasets (-> Dataset)\n",
        "    log.info('Padding tgt datasets...')\n",
        "    train_tgt = vocab.pad_sequences(train_tgt, max_len=opt.max_seq_len)\n",
        "    dev_tgt = vocab.pad_sequences(dev_tgt, max_len=opt.max_seq_len)\n",
        "    test_tgt = vocab.pad_sequences(test_tgt, max_len=opt.max_seq_len)\n",
        "    log.info('Done padding tgt datasets...')\n",
        "\n",
        "    # dataset loaders\n",
        "    log.info('Shuffling and batching...')\n",
        "    train_src = train_src.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size).shuffle(length['train_src']//opt.batch_size).shuffle(length['train_src']//opt.batch_size).shuffle(length['train_src']//opt.batch_size)\n",
        "    train_tgt = train_tgt.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size).shuffle(length['train_tgt']//opt.batch_size).shuffle(length['train_tgt']//opt.batch_size).shuffle(length['train_tgt']//opt.batch_size)\n",
        "    with tf.device('CPU'):\n",
        "        train_src_Q = tf.identity(train_src)\n",
        "        train_tgt_Q = tf.identity(train_src)\n",
        "    train_src_Q_iter = iter(train_src_Q)\n",
        "    train_tgt_Q_iter = iter(train_tgt_Q)\n",
        "    \n",
        "    dev_src = dev_src.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size)\n",
        "    dev_tgt = dev_tgt.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size)\n",
        "    \n",
        "    test_src = test_src.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size)\n",
        "    test_tgt = test_tgt.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size)\n",
        "    log.info('Done shuffling and batching.')\n",
        "\n",
        "    return vocab, train_src, dev_src, test_src, train_tgt, dev_tgt, test_tgt, train_src_Q, train_tgt_Q, train_src_Q_iter, train_tgt_Q_iter, length\n",
        "\n",
        "if __name__ == \"__main__\" and opt.notebook:\n",
        "    # clear dumps\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.keras.backend.set_learning_phase(0)\n",
        "    print(tf.keras.backend.learning_phase())\n",
        "    vocab, train_src, dev_src, test_src, train_tgt, dev_tgt, test_tgt, train_src_Q, train_tgt_Q, train_src_Q_iter, train_tgt_Q_iter, length = get_train_data(opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training ADAN with options:\n",
            "INFO:__main__:Namespace(F_bn=False, F_layers=1, P_bn=True, P_layers=1, Q_bn=True, Q_layers=1, Q_learning_rate=0.1, __version__=5, _lambda=0.1, activation='linear', attn='dot', avg_pooling=True, batch_size=50000, bidir_rnn=True, bos_tok='<s>', buffer_size=40000, ckpt_prefix=PosixPath('saved_models/lan_v5/ckpts'), clip_Q=False, clip_lim_FP=None, clipvalue=10.0, crash_logs=PosixPath('saved_models/lan_v5/crash_logs.txt'), current_ckpt=0, data_path=None, debug=False, device='cuda', dropout=0, emb_filename='', emb_size=300, eos_tok='</s>', epochs=5, fix_emb=False, fix_unk=False, hidden_size=900, iterate=False, kernel_num=400, kernel_sizes=[3, 4, 5], label_dtype=tf.int32, last_ckpt=0, learning_rate=0.1, logs=PosixPath('saved_models/lan_v5/logs.txt'), max_seq_len=100, model='lstm', n_vecs=-1, notebook=True, num_labels=6, pre_trained_src_emb_file='bwe/vectors/wiki.multi.en.vec', pre_trained_tgt_emb_file='bwe/vectors/wiki.multi.fr.vec', q_critic=5, random_emb=False, random_seed=1, saved_models=PosixPath('saved_models/lan_v5'), src_lang='en', start_fresh=False, tgt_lang='fr', train_size_src=-1, train_size_tgt=-1, unk_tok='<unk>', vector_length=10)\n",
            "INFO:__main__:Loading Embeddings...\n",
            "INFO:__main__:vocab initializing...done.\n",
            "INFO:__main__:reading pre-trained embeddings from bwe/vectors/wiki.multi.en.vec...\n",
            "  1%|          | 1164/200000 [00:00<00:17, 11632.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200000/200000 [00:17<00:00, 11310.93it/s]\n",
            "INFO:__main__:embedding vectors imported...\n",
            "INFO:__main__:reading pre-trained embeddings from bwe/vectors/wiki.multi.fr.vec...\n",
            "100%|██████████| 200000/200000 [00:17<00:00, 11328.19it/s]\n",
            "INFO:__main__:embedding vectors imported...\n",
            "INFO:__main__:Done.\n",
            "INFO:__main__:Loading src datasets...\n",
            "INFO:__main__:Reading 200000 lines from Amazon reviews/train/dataset_en_train.json\n",
            "100%|██████████| 200000/200000 [00:12<00:00, 15822.85it/s]\n",
            "INFO:__main__:Reading 5000 lines from Amazon reviews/dev/dataset_en_dev.json\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 40380.55it/s]\n",
            "INFO:__main__:Reading 5000 lines from Amazon reviews/test/dataset_en_test.json\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 38988.78it/s]\n",
            "INFO:__main__:Done loading src datasets.\n",
            "INFO:__main__:Loading tgt datasets...\n",
            "INFO:__main__:Reading 200000 lines from Amazon reviews/train/dataset_fr_train.json\n",
            "100%|██████████| 200000/200000 [00:06<00:00, 28956.57it/s]\n",
            "INFO:__main__:Reading 5000 lines from Amazon reviews/dev/dataset_fr_dev.json\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 37408.66it/s]\n",
            "INFO:__main__:Reading 5000 lines from Amazon reviews/test/dataset_fr_test.json\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 37320.92it/s]\n",
            "INFO:__main__:Done loading tgt datasets.\n",
            "INFO:__main__:Padding src datasets...\n",
            "100%|██████████| 200000/200000 [00:00<00:00, 2829267.37it/s]\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 2121335.22it/s]\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 2332760.85it/s]\n",
            "INFO:__main__:Done padding src datasets...\n",
            "INFO:__main__:Padding tgt datasets...\n",
            "100%|██████████| 200000/200000 [00:00<00:00, 2692972.07it/s]\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 1441638.83it/s]\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 1024250.06it/s]\n",
            "INFO:__main__:Done padding tgt datasets...\n",
            "INFO:__main__:Shuffling and batching...\n",
            "INFO:__main__:Done shuffling and batching.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3g7GTcGngcv",
        "colab_type": "text"
      },
      "source": [
        "## Custom layers\n",
        "- ### Averaging layer to average embeddings of Embedding layer (for CBOW approach)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39GTfkexjyur",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title layers.py\n",
        "\n",
        "#!/usr/bin/env ipython\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#from options import *\n",
        "#from vocab import *\n",
        "#from data import *\n",
        "#from utils import *\n",
        "\n",
        "class Averaging(layers.Layer):\n",
        "    def __init__(self, toks=None, vector_length=1, **kwargs):\n",
        "        super(Averaging, self).__init__(**kwargs)\n",
        "        self.toks = [opt.unk_idx, opt.bos_idx, opt.eos_idx] if toks is None else toks\n",
        "        self.vl = vector_length\n",
        "\n",
        "    def call(self, embeddings, lengths):\n",
        "        self.W = tf.cast(tf.reduce_sum(embeddings, axis=1), dtype=tf.float32)            # (BSZ, EMBDIM)\n",
        "        return self.W/tf.cast(tf.reshape(lengths, (-1, 1)), dtype=tf.float32) * self.vl   # (BSZ, EMBDIM)    #list(lengths.numpy())\n",
        "    \n",
        "if __name__ == \"__main__\" and not opt.notebook:\n",
        "    infile = 'Amazon reviews/test/dataset_en_test.json'\n",
        "    vocab = Vocab(opt.pre_trained_src_emb_file, vecs=10000)\n",
        "    rev = AmazonReviews()\n",
        "    data = rev.load_data(lang='en', dat='train', lines=100)\n",
        "    data = vocab.pad_sequences(data)\n",
        "    emb_layer = vocab.init_embed_layer()\n",
        "    avg = Averaging()\n",
        "    for x, l, y in data.take(10):\n",
        "        print(avg(emb_layer(x), l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad4iprJqkxCI",
        "colab_type": "text"
      },
      "source": [
        "## IF CRASHED previously:\n",
        "<br> <i>Run till here and jump to next MARKUP Checkpoint</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5TdFQm_rkPg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68416168-ef5f-4734-8dc0-f4c4de3a2ed1"
      },
      "source": [
        "#@title\n",
        "current_ckpt = 0\n",
        "TRAIN = current_ckpt == opt.last_ckpt\n",
        "print(f'TRAIN FRESH : {TRAIN}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN FRESH : True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv-J2djFn6hM",
        "colab_type": "text"
      },
      "source": [
        "## Custom models (CKPT0)\n",
        "- ### E - Embedding layer (from vocab).\n",
        "- ### A - Averaging layer (defined above).\n",
        "- ### EA - Model of above layers with reviews and review-lengths as inputs (keras.Input()).\n",
        "- ### F - DAN Feature Extractor (extract features from above EA model).\n",
        "- ### EAF - Overall DAN Feature Extractor (takes inputs as reviews and review lengths and gives features as outputs).\n",
        "- ### P - Semantic classifier over top of EAF model.\n",
        "- ### EAFP - Overall Semantic classifier (takes inputs as reviews and review lengths and gives softmax star-rating labels as outputs). Loss is taken to be sparse_categorical_crossentropy loss.\n",
        "- ### Q - Language Detector over top of EAF model.\n",
        "- ### EAFQ - Overall Language detector (takes inputs as reviews and review lengths and gives language score as output). Extracts language identification features without star-rating labels for adversarial training of F and EAF models. Loss is taken to be hinge loss.\n",
        "- ### LAN - Overall model with two branches namely EAFP and EAFQ wih shared EAF base and different tops P, Q."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0MthXomstTh",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1ca564f3-4543-4f3f-e207-a111e242722e"
      },
      "source": [
        "#@title models.py\n",
        "\n",
        "#!/usr/bin/env ipython\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, preprocessing, losses\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "import os, io\n",
        "from pathlib import Path\n",
        "os.chdir(os.path.dirname(__file__))\n",
        "\n",
        "#from options import opt\n",
        "#from layers import *\n",
        "\n",
        "def absexp_1(x):\n",
        "    return tf.clip_by_value(tf.math.expm1(tf.cast(tf.abs(x), dtype=tf.float64)) * tf.sign(x), -1.7e+308, 1.7e+308)\n",
        "\n",
        "def scce(y_true, y_pred):\n",
        "    return losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n",
        "\n",
        "def hinge(ll_lang, ll_pred):\n",
        "    return losses.Hinge()(ll_true, ll_pred)\n",
        "\n",
        "def total_loss(y_ll_true, y_ll_pred):\n",
        "    y_true, ll_true = zip(*y_ll_true)\n",
        "    y_pred, ll_pred = zip(*y_ll_pred)\n",
        "    return scce(y_true, y_pred) + opt._lambda * hinge(ll_true, ll_pred)\n",
        "\n",
        "class lstm_EA(keras.Model):\n",
        "    def __init__(self, E, A):\n",
        "        super(lstm_EA, self).__init__()\n",
        "        self.E = E\n",
        "        self.A = A\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.E(inputs)\n",
        "        return self.A(x)\n",
        "\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    TRAIN = True\n",
        "    opt.num_labels = 6\n",
        "    opt._lambda = 1.\n",
        "    opt.F_layers = 2\n",
        "    opt.P_layers = 2\n",
        "    opt.Q_layers = 2\n",
        "    opt.F_activation = 'relu'   # absexp_1\n",
        "    opt.P_activation = 'relu'\n",
        "    opt.Q_activation = 'relu'\n",
        "    opt.model = 'lstm' #'dan'\n",
        "    opt.lstm_hidden = 256\n",
        "\n",
        "    num_layers, hidden_size, dropout, batch_norm, activation = opt.F_layers, opt.hidden_size, opt.dropout, opt.F_bn, opt.F_activation\n",
        "    F = keras.Sequential()\n",
        "    for i in range(num_layers):\n",
        "        if dropout > 0: F.add(layers.Dropout(rate=dropout, name=f'Dropout_{i}'))\n",
        "        if i == 0:\n",
        "            if opt.model == 'dan': F.add(layers.Dense(units=hidden_size, input_shape=(vocab.emb_size,), activation=activation, name=f'DenseAbsExp_{i}'))\n",
        "            if opt.model == 'lstm': F.add(layers.Dense(units=hidden_size, input_shape=(opt.lstm_hidden*2,), activation=activation, name=f'DenseAbsExp_{i}'))\n",
        "        else: F.add(layers.Dense(units=hidden_size, input_shape=(hidden_size,), activation=activation, name=f'DenseAbsExp_{i}'))\n",
        "        if batch_norm: F.add(layers.BatchNormalization(input_shape=(hidden_size,), name=f'BatchNorm_{i}'))    # same shape as input    # use training=False when making inference from model (model.predict, model.evaluate?)\n",
        "    #F.add(layers.LeakyReLU(alpha=0.3))\n",
        "    #F.add(layers.ReLU())\n",
        "    F.add(layers.Dense(units=hidden_size, input_shape=(hidden_size,), activation=activation, name=f'DenseAbsExpFinal_{i}'))\n",
        "\n",
        "    num_layers, hidden_size, output_size, dropout, batch_norm, activation = opt.P_layers, opt.hidden_size, opt.num_labels, opt.dropout, opt.P_bn, opt.P_activation\n",
        "    P = models.Sequential()\n",
        "    P.add(keras.Input((opt.hidden_size,)))\n",
        "    for i in range(num_layers):\n",
        "        if dropout > 0: P.add(layers.Dropout(rate=dropout))\n",
        "        P.add(layers.Dense(units=hidden_size, input_shape=(hidden_size,), activation=opt.activation, name=f'DenseAbsExp_{i}'))\n",
        "        if batch_norm: P.add(layers.BatchNormalization())\n",
        "        #P.add(layers.ReLU())\n",
        "    #P.add(layers.Dense(units=output_size, input_shape=(hidden_size,), activation='tanh'))\n",
        "    P.add(layers.Dense(units=output_size, input_shape=(hidden_size,), activation='softmax', name=f'DenseSoftmax_{i}'))\n",
        "\n",
        "    num_layers, hidden_size, dropout, batch_norm, activation = opt.Q_layers, opt.hidden_size, opt.dropout, opt.Q_bn, opt.Q_activation\n",
        "    Q = keras.Sequential()\n",
        "    for i in range(num_layers):\n",
        "        if dropout > 0: Q.add(layers.Dropout(rate=dropout, name=f'Dropout_{i}'))\n",
        "        Q.add(layers.Dense(units=hidden_size, input_shape=(hidden_size,), activation=activation, name=f'DenseAbsExp_{i}'))\n",
        "        if batch_norm: Q.add(layers.BatchNormalization(input_shape=(hidden_size,), name=f'BathcNorm_{i}'))\n",
        "    Q.add(layers.Dense(units=hidden_size, input_shape=(hidden_size,), activation=activation, name=f'DenseAbsExpFinal_{i}'))\n",
        "    Q.add(layers.Dense(units=1, input_shape=(hidden_size,), activation='tanh', name=f'DenseTanh'))\n",
        "    #Q.add(layers.Softmax())\n",
        "\n",
        "    E = vocab.init_embed_layer()\n",
        "    if opt.model == 'dan': A = Averaging(toks=[vocab.unk_idx, vocab.bos_idx, vocab.eos_idx], vector_length=opt.vector_length)\n",
        "    if opt.model == 'lstm': A = layers.Bidirectional(layers.LSTM(opt.lstm_hidden))\n",
        "\n",
        "    shape = (opt.max_seq_len,)\n",
        "    inputs, lengths = keras.Input(shape), keras.Input(())\n",
        "\n",
        "    if opt.model == 'dan': embeddings = E(inputs); outputs_EA = A(embeddings, lengths); EA = keras.Model(inputs=[inputs, lengths], outputs=outputs_EA)\n",
        "    if opt.model == 'lstm': EA = lstm_EA(E, A); outputs_EA = EA(inputs)\n",
        "    \n",
        "    outputs_EAF = F(outputs_EA)\n",
        "    EAF = keras.Model(inputs=[inputs, lengths], outputs=outputs_EAF, name=\"FeatureExtractor_AE\")\n",
        "\n",
        "    outputs_EAFP = P(outputs_EAF)\n",
        "    EAFP = keras.Model(inputs=[inputs, lengths], outputs=outputs_EAFP, name=\"SemanticClassifier_FAE\")\n",
        "\n",
        "    outputs_EAFQ = Q(outputs_EAF)\n",
        "    EAFQ = keras.Model(inputs=[inputs, lengths], outputs=outputs_EAFQ, name=\"LanguageDetector_FAE\")\n",
        "\n",
        "    LAN = keras.Model(inputs=[inputs, lengths], outputs=[outputs_EAFP, outputs_EAFQ], name=\"LAN\")\n",
        "    save_models(current_ckpt, __version__=opt.__version__)\n",
        "else:\n",
        "    log.info(f' Skipping to ckpt {opt.last_ckpt}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__: Current ckpt : 0\n",
            "INFO:__main__: Running ckpt 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_models/lan_v5/ckpts/ckpt_0/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_models/lan_v5/ckpts/ckpt_0/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAvvbL1362fH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "294c7929-ff08-4d78-af48-e1d88b1613d9"
      },
      "source": [
        "LAN.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"LAN\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_ea_1 (lstm_EA)             (None, 512)          148132936   input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 900)          2083500     lstm_ea_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_4 (Sequential)       (None, 6)            1634406     sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_5 (Sequential)       (None, 1)            2440801     sequential_3[1][0]               \n",
            "==================================================================================================\n",
            "Total params: 154,291,643\n",
            "Trainable params: 7,292,243\n",
            "Non-trainable params: 146,999,400\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFXOdkaPqnUd",
        "colab_type": "text"
      },
      "source": [
        "## Checking if all models are defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnYOniDE1Jsk",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f99f946-b549-4acb-893a-c67897aca7f2"
      },
      "source": [
        "#@title train.py : setup\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    log.info('Checking outputs and initializing...')\n",
        "    E.trainable=False\n",
        "    EA.trainable=False\n",
        "    for (inputs, lengths, labels) in train_src.take(1).take(1):\n",
        "        print(inputs)\n",
        "        x = LAN([inputs[:25], lengths[:25]])\n",
        "        print((x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__: Current ckpt : 0\n",
            "INFO:__main__: Running ckpt 0\n",
            "INFO:__main__:Checking outputs and initializing...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[    0     0     0 ...    11    59  1412]\n",
            " [    0     0     0 ...   590  7446 62555]\n",
            " [    0     0     0 ...     6   971 12870]\n",
            " ...\n",
            " [    0     0     0 ...   935    19 10327]\n",
            " [    0     0     0 ...    18    85 32316]\n",
            " [    0     0     0 ...  9370  2337  4468]], shape=(50000, 100), dtype=int32)\n",
            "[<tf.Tensor: shape=(25, 6), dtype=float32, numpy=\n",
            "array([[0.16635773, 0.16427487, 0.16760135, 0.16666478, 0.16732521,\n",
            "        0.16777603],\n",
            "       [0.16684163, 0.16618922, 0.16702268, 0.16551854, 0.16653192,\n",
            "        0.16789599],\n",
            "       [0.16695976, 0.16491935, 0.16747016, 0.16667984, 0.1668722 ,\n",
            "        0.16709867],\n",
            "       [0.16619085, 0.16559781, 0.16749991, 0.1666818 , 0.1665267 ,\n",
            "        0.1675029 ],\n",
            "       [0.1662335 , 0.16555502, 0.16750893, 0.16586666, 0.1675242 ,\n",
            "        0.16731167],\n",
            "       [0.16589448, 0.1652389 , 0.16730711, 0.16709244, 0.1673276 ,\n",
            "        0.1671395 ],\n",
            "       [0.16613819, 0.16501752, 0.16747965, 0.1661885 , 0.16735493,\n",
            "        0.1678212 ],\n",
            "       [0.16634962, 0.16543938, 0.1677942 , 0.16617936, 0.16733533,\n",
            "        0.16690205],\n",
            "       [0.16625369, 0.16575208, 0.1673692 , 0.1665363 , 0.16644332,\n",
            "        0.16764544],\n",
            "       [0.16606343, 0.16567983, 0.16840924, 0.16601661, 0.16712502,\n",
            "        0.16670588],\n",
            "       [0.16636904, 0.16464964, 0.16720372, 0.16722156, 0.16751488,\n",
            "        0.16704118],\n",
            "       [0.1664692 , 0.16521661, 0.16679023, 0.16705783, 0.16769332,\n",
            "        0.16677278],\n",
            "       [0.16577749, 0.16517052, 0.16827092, 0.16608284, 0.16736162,\n",
            "        0.16733667],\n",
            "       [0.16595133, 0.16488992, 0.16803467, 0.16666642, 0.16651271,\n",
            "        0.16794495],\n",
            "       [0.16608885, 0.16602975, 0.16721699, 0.16704033, 0.16733006,\n",
            "        0.16629404],\n",
            "       [0.1656565 , 0.16610062, 0.16716616, 0.16635455, 0.16741863,\n",
            "        0.16730353],\n",
            "       [0.16718292, 0.16569896, 0.16708751, 0.16692385, 0.16645497,\n",
            "        0.16665184],\n",
            "       [0.16747375, 0.16388355, 0.16689572, 0.16695088, 0.1671166 ,\n",
            "        0.16767952],\n",
            "       [0.1659357 , 0.16616268, 0.1676163 , 0.16694409, 0.16644043,\n",
            "        0.1669008 ],\n",
            "       [0.16610955, 0.16488788, 0.16725518, 0.16735101, 0.16738896,\n",
            "        0.16700748],\n",
            "       [0.16551808, 0.16543674, 0.16800259, 0.16571078, 0.16772448,\n",
            "        0.16760737],\n",
            "       [0.16622488, 0.16559905, 0.16685154, 0.16700788, 0.16779144,\n",
            "        0.16652521],\n",
            "       [0.16545455, 0.16573443, 0.16896875, 0.16694666, 0.16608906,\n",
            "        0.16680658],\n",
            "       [0.166835  , 0.16513395, 0.16723213, 0.1671554 , 0.1668996 ,\n",
            "        0.16674395],\n",
            "       [0.16554454, 0.16534768, 0.16736723, 0.16677195, 0.16783871,\n",
            "        0.16712989]], dtype=float32)>, <tf.Tensor: shape=(25, 1), dtype=float32, numpy=\n",
            "array([[-9.6746255e-04],\n",
            "       [ 2.5168649e-04],\n",
            "       [-3.3513724e-04],\n",
            "       [ 3.9967729e-04],\n",
            "       [-5.8563444e-04],\n",
            "       [-1.1747552e-03],\n",
            "       [-9.5637428e-04],\n",
            "       [-4.9704377e-04],\n",
            "       [ 4.3863113e-04],\n",
            "       [-8.5534836e-04],\n",
            "       [-1.3407617e-03],\n",
            "       [ 4.4443712e-04],\n",
            "       [-1.2864354e-03],\n",
            "       [-2.4423725e-04],\n",
            "       [ 8.8113244e-05],\n",
            "       [ 3.2564602e-04],\n",
            "       [ 1.6792747e-04],\n",
            "       [-1.3898810e-03],\n",
            "       [-6.5939559e-05],\n",
            "       [-9.7334513e-04],\n",
            "       [-6.0505373e-04],\n",
            "       [-3.6578809e-04],\n",
            "       [ 4.6099329e-04],\n",
            "       [ 5.2129611e-04],\n",
            "       [-5.4801279e-04]], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNNZWXV3q6ib",
        "colab_type": "text"
      },
      "source": [
        "## Set training rates and other train statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7MuBjtvD1_e",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "91bbc109-577c-4128-e900-42c441801657"
      },
      "source": [
        "#@title Training statistics : learning_rates\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    opt.learning_rate, opt.Q_learning_rate = 1e-2, 1e-3  # 1e-3 is default\n",
        "    opt.learning_rate, opt.Q_learning_rate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__: Current ckpt : 0\n",
            "INFO:__main__: Running ckpt 0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJuBfuEHrGo8",
        "colab_type": "text"
      },
      "source": [
        "# Train inner models (EAFP, EAFQ) without/with embeddings-training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3ne3CZvrNO1",
        "colab_type": "text"
      },
      "source": [
        "## Training without training the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxjDR8LHgCAy",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fd964848-5ca1-4692-8143-63072d99fabc"
      },
      "source": [
        "#@title Setting the embeddings non-trainable\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    E.trainable = False\n",
        "    EA.trainable = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__: Current ckpt : 0\n",
            "INFO:__main__: Running ckpt 0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3QWMaDesrWq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "938d61a5-85b2-48e6-be80-1e68be57fc40"
      },
      "source": [
        "#@title EA layers\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    print(\"Trainable EA layers : \")\n",
        "    print(*[x.name + '\\n' for x in EA.trainable_variables])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__: Current ckpt : 0\n",
            "INFO:__main__: Running ckpt 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Trainable EA layers : \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUklDOhP35BY",
        "colab_type": "text"
      },
      "source": [
        "### Training F and P on labeled source data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tdzl8nxcZLw",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "86d98a97-0498-48fe-ae1c-b40331ed6e1f"
      },
      "source": [
        "#@title Trainable layers for EAFP training with fixed embeddings\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    print(\"Trainable EAFP layers : \")\n",
        "    print(*[x.name + '\\n' for x in EAFP.trainable_variables])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__: Current ckpt : 0\n",
            "INFO:__main__: Running ckpt 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Trainable layers : \n",
            "DenseAbsExp_0/kernel:0\n",
            " DenseAbsExp_0/bias:0\n",
            " DenseAbsExp_1/kernel:0\n",
            " DenseAbsExp_1/bias:0\n",
            " DenseAbsExpFinal_1/kernel:0\n",
            " DenseAbsExpFinal_1/bias:0\n",
            " DenseAbsExp_0_1/kernel:0\n",
            " DenseAbsExp_0_1/bias:0\n",
            " batch_normalization/gamma:0\n",
            " batch_normalization/beta:0\n",
            " DenseAbsExp_1_1/kernel:0\n",
            " DenseAbsExp_1_1/bias:0\n",
            " batch_normalization_1/gamma:0\n",
            " batch_normalization_1/beta:0\n",
            " DenseSoftmax_1/kernel:0\n",
            " DenseSoftmax_1/bias:0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWwpWjduBfBt",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dadffe7c-7b19-419d-d39f-4172d4e7869a"
      },
      "source": [
        "#@title train.py : Training F and P : sparse categorical\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info('Training EAFP model with src data with fixed embeddings...')\n",
        "    TRAIN = True\n",
        "    EAFP.compile(optimizer=optimizers.Adam(opt.learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    epochs = 20\n",
        "    for epoch in trange(epochs):\n",
        "        batch_no = 0\n",
        "        for (inputs, lengths, labels) in train_src:\n",
        "            log.info(f\"Training on batch no : {batch_no}\"); batch_no += 1\n",
        "            print(scce(labels, EAFP.predict([inputs, lengths])))\n",
        "            history = EAFP.fit(x=[inputs, lengths], y=labels, epochs=1)\n",
        "else:\n",
        "    log.info(f' Skipping to ckpt {opt.last_ckpt}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training EAFP model with src data with fixed embeddings...\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]INFO:__main__:Training on batch no : 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.0979762, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6690 - accuracy: 0.6678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(11.406458, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8463 - accuracy: 0.6773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.0505176, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0244 - accuracy: 0.5454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.4517825, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0654 - accuracy: 0.5476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [00:38<05:42, 38.09s/it]INFO:__main__:Training on batch no : 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.4664485, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6712 - accuracy: 0.6711\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.688727, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0708 - accuracy: 0.5470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.3015093, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7908 - accuracy: 0.6845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.1236005, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0309 - accuracy: 0.5427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [01:16<05:04, 38.12s/it]INFO:__main__:Training on batch no : 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.0197755, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0120 - accuracy: 0.5485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.1293217, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6648 - accuracy: 0.6696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(8.494419, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8368 - accuracy: 0.6786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.3891369, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0546 - accuracy: 0.5517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [01:54<04:26, 38.12s/it]INFO:__main__:Training on batch no : 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.4522464, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0216 - accuracy: 0.5440\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.6127083, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0514 - accuracy: 0.5499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.0470965, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6722 - accuracy: 0.6706\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(7.4131875, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8253 - accuracy: 0.6784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [02:31<03:47, 37.90s/it]INFO:__main__:Training on batch no : 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.266635, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6723 - accuracy: 0.6710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(7.3578963, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8132 - accuracy: 0.6850\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.447246, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0607 - accuracy: 0.5494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.5479605, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0234 - accuracy: 0.5453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [03:09<03:08, 37.71s/it]INFO:__main__:Training on batch no : 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(5.4638786, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8033 - accuracy: 0.6807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.9015784, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0142 - accuracy: 0.5493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.4507102, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6638 - accuracy: 0.6713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(5.9939814, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0766 - accuracy: 0.5477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [03:46<02:30, 37.55s/it]INFO:__main__:Training on batch no : 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.4065663, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0089 - accuracy: 0.5487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(4.9835525, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7955 - accuracy: 0.6854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.7679482, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0420 - accuracy: 0.5570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.8287073, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6668 - accuracy: 0.6747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [04:23<01:52, 37.47s/it]INFO:__main__:Training on batch no : 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.2496502, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0118 - accuracy: 0.5503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.1353931, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6569 - accuracy: 0.6758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.33355, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0731 - accuracy: 0.5528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.1739274, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7824 - accuracy: 0.6880\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [05:00<01:14, 37.40s/it]INFO:__main__:Training on batch no : 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.9463065, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6694 - accuracy: 0.6740\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(8.403974, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8180 - accuracy: 0.6825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.4114872, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0381 - accuracy: 0.5550\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.5895053, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0113 - accuracy: 0.5473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [05:38<00:37, 37.40s/it]INFO:__main__:Training on batch no : 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.628117, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0446 - accuracy: 0.5543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.286002, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6707 - accuracy: 0.6732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(7.2285724, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8116 - accuracy: 0.6851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Training on batch no : 3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.2546685, shape=(), dtype=float32)\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0175 - accuracy: 0.5474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [06:15<00:00, 37.55s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQgwKu-W8-o9",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation of Sentiment Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbcAjVihckb7",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "0b8c0fd2-3d18-4ac2-dd2e-e142e692c8d7"
      },
      "source": [
        "#@title  P results : Unseen target data loss and  accuracy\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    for inputs, lengths, labels in train_src:\n",
        "        print(\"SRC test : \\n\", EAFP.evaluate(x=[inputs, lengths], y=labels))\n",
        "    for inputs, lengths, labels in train_tgt:\n",
        "        print(\"TGT test : \\n\", EAFP.evaluate(x=[inputs, lengths], y=labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__: Current ckpt : 0\n",
            "INFO:__main__: Running ckpt 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0029 - accuracy: 0.5553\n",
            "SRC test : \n",
            " [1.0028516054153442, 0.5553399920463562]\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.8464 - accuracy: 0.2699\n",
            "SRC test : \n",
            " [2.8464269638061523, 0.26989999413490295]\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1005 - accuracy: 0.4295\n",
            "SRC test : \n",
            " [1.1005363464355469, 0.429500013589859]\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 5.9029 - accuracy: 0.1145\n",
            "SRC test : \n",
            " [5.902913570404053, 0.11445999890565872]\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1925 - accuracy: 0.4787\n",
            "TGT test : \n",
            " [1.1924664974212646, 0.47874000668525696]\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 5.7750 - accuracy: 0.0891\n",
            "TGT test : \n",
            " [5.7750468254089355, 0.08910000324249268]\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 3.0413 - accuracy: 0.2161\n",
            "TGT test : \n",
            " [3.0413401126861572, 0.21605999767780304]\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5467 - accuracy: 0.1336\n",
            "TGT test : \n",
            " [1.5466797351837158, 0.1336199939250946]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQTwwLpB76L_",
        "colab_type": "text"
      },
      "source": [
        "### Training F and Q on language-labeled source-target data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r4VVtCXXrMn",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Trainable layers for EAFQ training with fixed embeddings\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    [x.name for x in EAFQ.trainable_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omXqFmLlJ53a",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title train.py : Training F and Q : adversarial : hinge loss\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    log.info('Training EAFQ model with src and tgt data with lang_labels and fixed embeddings...')\n",
        "    TRAIN = True\n",
        "    #opt.Q_learning_rate = 1e-4\n",
        "    EAFQ.compile(optimizer=optimizers.Adam(opt.Q_learning_rate), loss='hinge', metrics=['accuracy'])\n",
        "    opt.Q_iterations = 5\n",
        "    for epoch in trange(opt.Q_iterations):\n",
        "        batch_no = 0\n",
        "        for (inputs_src, lengths_src, labels_src), (inputs_tgt, lengths_tgt, labels_tgt) in zip(train_src, train_tgt):\n",
        "            log.info(f\"Training on batch no : {batch_no}\"); batch_no += 1\n",
        "            inputs = tf.concat([inputs_src, inputs_tgt], axis=0)\n",
        "            lengths = tf.concat([lengths_src, lengths_tgt], axis=0)\n",
        "            lang_labels_src = tf.broadcast_to([1], shape=labels_src.shape)\n",
        "            lang_labels_tgt = tf.broadcast_to([-1], shape=labels_tgt.shape)\n",
        "            lang_labels = tf.concat([lang_labels_src, lang_labels_tgt], axis=0)\n",
        "            #print(lang_labels)\n",
        "            history = EAFQ.fit(x=[inputs, lengths], y=lang_labels, epochs=1)\n",
        "else:\n",
        "    log.info(f' Skipping to ckpt {opt.last_ckpt}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ic2Hjrb8tj3",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation of Sentiment Classifier and language detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxOKkNwjPmd1",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title  P results : Unseen target data loss and  accuracy after F-Q training\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    EAFP.evaluate([inputs_tgt, lengths_tgt], labels_tgt, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUIKfv2IOZL9",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Q results\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    EAFQ.compile(optimizer=optimizers.Adam(opt.Q_learning_rate), loss='hinge', metrics=['accuracy', keras.metrics.Hinge()])\n",
        "    print(EAFQ.predict([inputs_src, lengths_src]), '\\n', EAFQ.evaluate([inputs_src, lengths_src], lang_labels_src, verbose=2), '\\n', EAFQ.predict([inputs_tgt, lengths_tgt]), '\\n', EAFQ.evaluate([inputs_tgt, lengths_tgt], lang_labels_tgt, verbose=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptsreRts8DWK",
        "colab_type": "text"
      },
      "source": [
        "## Save the trained models before embedding training (CKPT1)\n",
        "<br>Continue by loading from this saved checkpoint.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_dphA20BUw9",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Save/Load model and weights\n",
        "current_ckpt += 1\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    E, A, F, P, Q, EA, EAF, EAFP, EAFQ, LAN = load_models(current_ckpt, __version__=opt.__version__)\n",
        "elif opt.last_ckpt < current_ckpt:\n",
        "    log.info(\"Done/Overstepped last_ckpt before...\")\n",
        "    #input(\"Interrupt execution to not save else enter anything : \")\n",
        "    save_models(current_ckpt, __version__=opt.__version__)\n",
        "    with open(opt.crash_logs, 'w') as foo: foo.write(str(current_ckpt))\n",
        "    if last_ckpt != -1 and last_ckpt < current_ckpt:\n",
        "else:\n",
        "    log.info(f' Skipping to ckpt {opt.last_ckpt}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jJYmd_G8jeD",
        "colab_type": "text"
      },
      "source": [
        "## Training with trainable embeddings\n",
        "<br> <i>Each step will take time so keep saving the models and don't change tab for long either</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GvG6a4DfyGp",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Setting the embeddings trainable\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    E.trainable = True\n",
        "    EA.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2EK5gwh9Wh7",
        "colab_type": "text"
      },
      "source": [
        "### Training Sentiment Classifier (EAFP) on labeled source data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNAYQdiucflO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Trainable layers for EAFP training with trainable embeddings\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    [x.name for x in EAFP.trainable_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dar-cFfxNiby",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title train.py : Training Embeddings, F and P : sparse categorical\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    log.info('Training EAFP model with src data with fixed embeddings...')\n",
        "    TRAIN = True\n",
        "    EAFP.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    epochs = 1\n",
        "    for epoch in trange(epochs):\n",
        "        batch_no = 0\n",
        "        for (inputs, lengths, labels) in train_src:\n",
        "            log.info(f\" Training on batch no : {batch_no}\"); batch_no += 1\n",
        "            print(scce(labels, EAFP.predict([inputs, lengths])))\n",
        "            history = EAFP.fit(x=[inputs, lengths], y=labels, epochs=1)\n",
        "else:\n",
        "    log.info(f' Skipping to ckpt {opt.last_ckpt}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bPOtstASFyIZ"
      },
      "source": [
        "## Save the trained models after embedding training of EAFP (CKPT2)\n",
        "<br>Continue by loading from this saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "S2RiSO-gFyIb",
        "colab": {}
      },
      "source": [
        "#@title Save/Load model and weights\n",
        "current_ckpt += 1\n",
        "log.info(f' Reached ckpt {current_ckpt}')\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    E, A, F, P, Q, EA, EAF, EAFP, EAFQ, LAN = load_models(current_ckpt, __version__=opt.__version__)\n",
        "elif opt.last_ckpt < current_ckpt:\n",
        "    log.info(\"Done/Overstepped last_ckpt before...\")\n",
        "    #input(\"Interrupt execution to not save else enter anything : \")\n",
        "    save_models(current_ckpt, __version__=opt.__version__)\n",
        "    with open(opt.crash_logs, 'w') as foo: foo.write(str(current_ckpt))\n",
        "    if last_ckpt != -1 and last_ckpt < current_ckpt:\n",
        "else:\n",
        "    log.info(f' Skipping to ckpt {opt.last_ckpt}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN4Kslf4-Pnz",
        "colab_type": "text"
      },
      "source": [
        "###Training Language Detector (EAFQ) on language-labeled target data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UujQSmNJdKaK",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Trainable layers for EAFQ training with trainable embeddings\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    [x.name for x in EAFQ.trainable_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b--azhiggzXX",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title train.py : Training Embeddings, F and Q  : hinge loss\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    log.info('Training EAFQ model with src and tgt data with lang_labels and fixed embeddings...')\n",
        "    TRAIN = True\n",
        "    #opt.Q_learning_rate = 1e-5\n",
        "    EAFQ.compile(optimizer=optimizers.Adam(opt.Q_learning_rate), loss='hinge', metrics=['accuracy'])\n",
        "    opt.Q_iterations = 1\n",
        "    for epoch in trange(opt.Q_iterations):\n",
        "        batch_no = 0\n",
        "        for (inputs_src, lengths_src, labels_src), (inputs_tgt, lengths_tgt, labels_tgt) in zip(train_src, train_tgt):\n",
        "            log.info(f\" Training on batch no : {batch_no}\"); batch_no += 1\n",
        "            inputs = tf.concat([inputs_src, inputs_tgt], axis=0)\n",
        "            lengths = tf.concat([lengths_src, lengths_tgt], axis=0)\n",
        "            lang_labels_src = tf.broadcast_to([1], shape=labels_src.shape)\n",
        "            lang_labels_tgt = tf.broadcast_to([-1], shape=labels_tgt.shape)\n",
        "            lang_labels = tf.concat([lang_labels_src, lang_labels_tgt], axis=0)\n",
        "            history = EAFQ.fit(x=[inputs, lengths], y=lang_labels, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rAOT-UFDGC-D"
      },
      "source": [
        "## Save the trained models after embedding training of EAFQ (CKPT3)\n",
        "<br>Continue by loading from this saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "3VumnADbGC-F",
        "colab": {}
      },
      "source": [
        "#@title Save/Load model and weights\n",
        "current_ckpt += 1\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    E, A, F, P, Q, EA, EAF, EAFP, EAFQ, LAN = load_models(current_ckpt, __version__=opt.__version__)\n",
        "elif opt.last_ckpt < current_ckpt:\n",
        "    log.info(\"Done/Overstepped last_ckpt before...\")\n",
        "    #input(\"Interrupt execution to not save else enter anything : \")\n",
        "    save_models(current_ckpt, __version__=opt.__version__)\n",
        "    with open(opt.crash_logs, 'w') as foo: foo.write(str(current_ckpt))\n",
        "    if last_ckpt != -1 and last_ckpt < current_ckpt:\n",
        "else:\n",
        "    log.info(f' Skipping to ckpt {opt.last_ckpt}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6oKLd4K6vOH",
        "colab_type": "text"
      },
      "source": [
        "# Training LAN with traininable embeddings\n",
        "<br><i>Will take a day<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9WhKwvRg6nZ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Trainable layers for LAN training with trainable embeddings\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    [x.name for x in LAN.trainable_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X6Dw6XyhNhr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title train.py : Training all - EA, F, P and Q : sparse categorical + adversarial : total loss\n",
        "#opt.Q_learning_rate = 1e-5\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    log.info('Training EAFQ model with src and tgt data with lang_labels and fixed embeddings...')\n",
        "    EAFQ.compile(optimizer=optimizers.Adam(opt.learning_rate), loss='hinge', metrics=['accuracy'])\n",
        "    LAN.compile(optimizer=optimizers.Adam(opt.learning_rate), loss=total_loss, metrics=['accuracy'])\n",
        "    opt.Q_iterations = 5\n",
        "    for epoch in trange(opt.Q_iterations):\n",
        "        batch_no = 0\n",
        "        for (inputs_src, lengths_src, labels_src), (inputs_tgt, lengths_tgt, labels_tgt) in zip(train_src, train_tgt):\n",
        "            log.info(f\" Training on batch no : {batch_no}\"); batch_no += 1\n",
        "            inputs = tf.concat([inputs_src, inputs_tgt], axis=0)\n",
        "            lengths = tf.concat([lengths_src, lengths_tgt], axis=0)\n",
        "            labels = tf.concat([labels_src, labels_tgt], axis=0)\n",
        "            lang_labels_src = tf.broadcast_to([1], shape=labels_src.shape)\n",
        "            lang_labels_tgt = tf.broadcast_to([-1], shape=labels_tgt.shape)\n",
        "            lang_labels = tf.concat([lang_labels_src, lang_labels_tgt], axis=0)\n",
        "            history_EAFQ = EAFQ.fit(x=[inputs, lengths], y=lang_labels, epochs=1)\n",
        "            history_LAN = LAN.fit(x=[inputs_src, lengths_src], y=[labels_src, lang_labels_src], epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCdu2up4L0m5",
        "colab_type": "text"
      },
      "source": [
        "## Save the trained models after embedding training of LAN (CKPT4)\n",
        "<br>Continue by loading from this saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRPJu2w-MCwk",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Save/Load model and weights\n",
        "current_ckpt += 1\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    E, A, F, P, Q, EA, EAF, EAFP, EAFQ, LAN = load_models(current_ckpt, __version__=opt.__version__)\n",
        "elif opt.last_ckpt < current_ckpt:\n",
        "    log.info(\"Done/Overstepped last_ckpt before...\")\n",
        "    #input(\"Interrupt execution to not save else enter anything : \")\n",
        "    save_models(current_ckpt, __version__=opt.__version__)\n",
        "    with open(opt.crash_logs, 'w') as foo: foo.write(str(current_ckpt))\n",
        "    if last_ckpt != -1 and last_ckpt < current_ckpt:\n",
        "else:\n",
        "    log.info(f' Skipping to ckpt {opt.last_ckpt}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0UuACCKAe3c",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Sentiment classifier and Language Detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-atO5MZiS-X",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Sentiment classifier results on unseen target data\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    for inputs, lengths, labels in test_src:\n",
        "        print(\"SRC test : \\n\", EAFP.evaluate(x=[], y=labels_tgt))\n",
        "    for inputs, lengths, labels in test_tgt:\n",
        "        print(\"TGT test : \\n\", EAFP.evaluate(x=[], y=labels_tgt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ktHT7MCBEqI",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Language Detector results\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    for (inputs_src, lengths_src, labels_src), (inputs_tgt, lengths_tgt, labels_tgt) in zip(test_src, test_tgt):\n",
        "        log.info(f\" LD test : \")\n",
        "        inputs = tf.concat([inputs_src, inputs_tgt], axis=0)\n",
        "        lengths = tf.concat([lengths_src, lengths_tgt], axis=0)\n",
        "        labels = tf.concat([labels_src, labels_tgt], axis=0)\n",
        "        lang_labels_src = tf.broadcast_to([1], shape=labels_src.shape)\n",
        "        lang_labels_tgt = tf.broadcast_to([-1], shape=labels_tgt.shape)\n",
        "        lang_labels = tf.concat([lang_labels_src, lang_labels_tgt], axis=0)\n",
        "        print(EAFQ.evaluate(x=[inputs, lengths], y=lang_labels, epochs=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiKJvJma7O2U",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate trained LAN and other models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv4HTDiyla0A",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Overall LAN results on unseen target data\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    for (inputs_src, lengths_src, labels_src), (inputs_tgt, lengths_tgt, labels_tgt) in zip(test_src, test_tgt):\n",
        "        log.info(f\" LAN test : \")\n",
        "        inputs = tf.concat([inputs_src, inputs_tgt], axis=0)\n",
        "        lengths = tf.concat([lengths_src, lengths_tgt], axis=0)\n",
        "        labels = tf.concat([labels_src, labels_tgt], axis=0)\n",
        "        lang_labels_src = tf.broadcast_to([1], shape=labels_src.shape)\n",
        "        lang_labels_tgt = tf.broadcast_to([-1], shape=labels_tgt.shape)\n",
        "        lang_labels = tf.concat([lang_labels_src, lang_labels_tgt], axis=0)\n",
        "        print(LAN.evaluate(x=[inputs_src, lengths_src], y=[labels_src, lang_labels_src], epochs=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qJFkvnZBvbQ",
        "colab_type": "text"
      },
      "source": [
        "# Save models and weights (CKPT_FINAL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCllyq6DBuBn",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Save model and weights\n",
        "#@title Save/Load model and weights\n",
        "current_ckpt = 'FINAL'\n",
        "log.info(f' Current ckpt : {current_ckpt}')\n",
        "if opt.last_ckpt == current_ckpt:\n",
        "    log.info(f' Running ckpt {current_ckpt}')\n",
        "    E, A, F, P, Q, EA, EAF, EAFP, EAFQ, LAN = load_models(current_ckpt, __version__=opt.__version__)\n",
        "elif TRAIN:\n",
        "    log.info(\"Done/Overstepped last_ckpt before...\")\n",
        "    #input(\"Interrupt execution to not save else enter anything : \")\n",
        "    save_models(current_ckpt, __version__=opt.__version__)\n",
        "    with open(opt.crash_logs, 'w') as foo: foo.write(str(current_ckpt))\n",
        "    if last_ckpt != -1 and last_ckpt < current_ckpt:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMYHH0f5A5oj",
        "colab_type": "text"
      },
      "source": [
        "# Extra (DO NOT RUN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Y7LCFzaBEc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title train.py (original thesis model - incomplete) - DO NOT RUN\n",
        "\n",
        "#!/usr/bin/env ipython\n",
        "# foo = open('train.py', 'r'); foo.readline(); exec(foo.read()); foo.close()\n",
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as functional\n",
        "#import torch.optim as optim\n",
        "#from torch.utils.data import DataLoader\n",
        "#from torchnet.meter import ConfusionMeter\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers, losses\n",
        "import json\n",
        "\n",
        "import os, random, sys, logging, argparse\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "os.chdir(os.path.dirname(__file__))\n",
        "\n",
        "#from options import *\n",
        "#from data import *\n",
        "#from vocab import *\n",
        "#from utils import *\n",
        "#from models import *\n",
        "#from train_data import *\n",
        "\n",
        "#tf.logging.set_verbosity(tf.logging.INFO)\n",
        "#tf.logging.set_verbosity(True)\n",
        "\n",
        "#random.seed(opt.random_seed)\n",
        "#torch.manual_seed(opt.random_seed)\n",
        "\n",
        "# save logs\n",
        "if not os.path.exists(opt.model_save_file): os.makedirs(opt.model_save_file)\n",
        "logging.basicConfig(stream=sys.stderr, level=logging.DEBUG if opt.debug else logging.INFO)\n",
        "log = logging.getLogger(__name__)\n",
        "fh = logging.FileHandler(os.path.join(opt.model_save_file, 'log.txt'))\n",
        "log.addHandler(fh)\n",
        "\n",
        "#vars = ['vocab', 'train_src', 'dev_src', 'test_src', 'train_tgt', 'dev_tgt', 'test_tgt', 'train_src_Q', 'train_tgt_Q', 'train_src_Q_iter', 'train_tgt_Q_iter', 'length']\n",
        "#for var in vars:\n",
        "#    if var not in locals() and var not in globals(): print(var, 'not imported '); exit()\n",
        "#    print(var, 'imported')\n",
        "\n",
        "\n",
        "#def train(opt):\n",
        "if __name__ == \"__main__\" and not TRAIN1:\n",
        "    \"\"\"Train Process:\n",
        "Require => labeled SOURCE corpus Xsrc; unlabeled TARGET corpus Xtgt; Hyperpamameter λ > 0, k ∈ N, c > 0; Lp(ˆy, y) crossentropy loss.\n",
        "=> Main iteration\n",
        "repeat\n",
        "    => Q iterations\n",
        "    for qiter = 1 to k do\n",
        "        Sample unlabeled batch x_src ~ X_src\n",
        "        Sample unlabeled batch x_tgt ~ X_tgt\n",
        "        f_src = F (x_src)\n",
        "        f_tgt = F (x_tgt) . feature vectors\n",
        "        lossq = -Q(f_src) + Q(f_tgt)\n",
        "        Update Q parameters to minimize lossq\n",
        "        ClipWeights(Q, -c, c)\n",
        "    \n",
        "    => F&P iteration\n",
        "    Sample labeled batch (x_src, y_src) ~ Xsrc\n",
        "    Sample unlabeled batch xtgt ~ Xtgt\n",
        "    f_src = F (x_src)\n",
        "    f_tgt = F (x_tgt)\n",
        "    loss = Lp(P(f_src); y_src) + λ * (Q(f_src) - Q(f_tgt))\n",
        "    Update F , P parameters to minimize loss\n",
        "until convergence\n",
        "    \"\"\"\n",
        "    DEBUG = lambda x : print('__DEBUG__ : ', x)\n",
        "    # data\n",
        "    if not opt.notebook: vocab, train_src, dev_src, test_src, train_tgt, dev_tgt, test_tgt, train_src_Q, train_tgt_Q, train_src_Q_iter, train_tgt_Q_iter, length = get_train_data(opt)\n",
        "\n",
        "    # models\n",
        "    log.info('Initializing models...')\n",
        "    if opt.model.lower() == 'dan': F = DAN_Feature_Extractor(vocab, opt.F_layers, opt.hidden_size, opt.dropout, opt.F_bn)\n",
        "    elif opt.model.lower() == 'lstm': F = LSTM_Feature_Extractor(vocab, opt.F_layers, opt.hidden_size, opt.dropout, opt.bdrnn, opt.attn)\n",
        "    elif opt.model.lower() == 'cnn': F = CNN_Feature_Extractor(vocab, opt.F_layers, opt.hidden_size, opt.kernel_num, opt.kernel_sizes, opt.dropout)\n",
        "    else: raise Exception('Unknown model')\n",
        "\n",
        "    P = Sentiment_Classifier(opt.P_layers, opt.hidden_size, opt.num_labels, opt.dropout, opt.P_bn)\n",
        "    Q = Language_Detector(opt.Q_layers, opt.hidden_size, opt.dropout, opt.Q_bn)\n",
        "    log.info('Done...')\n",
        "\n",
        "    optimizer_FP = Optimizer_FP(models=[F, P, Q], lr=opt.learning_rate, clip_lim=opt.clip_lim_FP)\n",
        "    if not opt.clip_Q: optimizer_Q = optimizers.Adam(lr=opt.Q_learning_rate)\n",
        "    else: optimizer_Q = optimizers.Adam(lr=opt.Q_learning_rate, clipvalue=opt.clipvalue)\n",
        "    \n",
        "    F.fcnet.compile(optimizer=optimizer_FP)\n",
        "    P.net.compile(optimizer=optimizer_FP)\n",
        "    Q.compile(optimizer=optimizer_Q)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    # train tgt iterator\n",
        "    train_tgt_iter = iter(train_tgt)\n",
        "    log.info('Main Iteration begin...')\n",
        "    \"\"\" Main iterations \"\"\"\n",
        "    for epoch in trange(opt.epochs):\n",
        "        F.unfreeze()\n",
        "        P.unfreeze()\n",
        "        Q.unfreeze()\n",
        "        F.freeze_emb_layer()\n",
        "        \n",
        "        # for training accuracy\n",
        "        correct, total = 0, 0\n",
        "        sum_src_q, sum_tgt_q = (0, 0.0), (0, 0.0)    # qiter number, loss_q\n",
        "        grad_norm_p, grad_norm_q = (0, 0.0), (0, 0.0)\n",
        "        \n",
        "        # train src iterator\n",
        "        train_src_iter = iter(train_src)\n",
        "        log.info('Q iteration begin...')\n",
        "        for i, (inputs_src, lengths_src, labels_src) in tqdm(enumerate(train_src_iter), total=(length['train_src'] + opt.batch_size - 1)//opt.batch_size):\n",
        "            \"\"\" sample batches: labeled (xsrc, ysrc) in Xsrc \"\"\"\n",
        "            \"\"\" sample unlabeled xtgt in Xtgt \"\"\"\n",
        "            try:\n",
        "                inputs_tgt, _, _ = next(train_tgt_iter)  # tgt labels not used\n",
        "            except:\n",
        "                # check if tgt data is exhausted\n",
        "                train_tgt_iter = iter(train_tgt)\n",
        "                inputs_tgt, _, _ = next(train_tgt_iter)\n",
        "            \n",
        "            \"\"\" Q iterations: \"\"\"\n",
        "            q_critic = 1 #opt.q_critic\n",
        "            #if q_critic>0 and ((epoch==0 and i<=25) or (i%500==0)): q_critic = 10\n",
        "            #F.freeze()\n",
        "            #P.freeze()\n",
        "            #Q.unfreeze()\n",
        "            #F.freeze_emb_layer()\n",
        "            #Q.clip_weights()\n",
        "\n",
        "            for qiter in range(q_critic):\n",
        "                \"\"\" sample unlabeled batches: xsrc in Xsrc, xtgt in Xtgt \"\"\"\n",
        "                # get a minibatch of data\n",
        "                try:\n",
        "                    # labels are not used\n",
        "                    inputs_src_Q, lengths_src_Q, _ = next(train_src_Q_iter)\n",
        "                except StopIteration:\n",
        "                    # check if dataloader is exhausted\n",
        "                    train_src_Q_iter = iter(train_src_Q)\n",
        "                    inputs_src_Q, lengths_src_Q, _ = next(train_src_Q_iter)\n",
        "                try:\n",
        "                    inputs_tgt_Q, lengths_tgt_Q, _ = next(train_tgt_Q_iter)\n",
        "                except StopIteration:\n",
        "                    train_tgt_Q_iter = iter(train_tgt_Q)\n",
        "                    inputs_tgt_Q, lengths_tgt_Q, _ = next(train_tgt_Q_iter)\n",
        "                \n",
        "                DEBUG(\"\"\" extract features : f_src, f_tgt = F(x_src), F(x_tgt) \"\"\")\n",
        "                features_src = F(inputs_src_Q, lengths_src_Q)\n",
        "                features_tgt = F(inputs_tgt_Q, lengths_tgt_Q)\n",
        "                \n",
        "                \"\"\" calculate loss_q : loss_q = -Q(f_src) + Q(f_tgt) \"\"\"\n",
        "                DEBUG(\"\"\" update Q to minimise loss_q \"\"\")\n",
        "                l_src_ad = Q.train_step(features_src, 'src', loss='scce')['loss']\n",
        "                l_tgt_ad = Q.train_step(features_tgt, 'tgt', loss='scce')['loss']\n",
        "                # summed Q losses\n",
        "                #sum_src_q = (sum_src_q[0] + 1, sum_src_q[1] + l_src_ad)\n",
        "                #sum_tgt_q = (sum_tgt_q[0] + 1, sum_tgt_q[1] + l_tgt_ad)\n",
        "\n",
        "                DEBUG(\"\"\" clip Q weights \"\"\")\n",
        "                #Q.clip_weights()\n",
        "            \n",
        "            log.info('Q iteration done.')\n",
        "\n",
        "            \"\"\" F&P iteration \"\"\"\n",
        "            #F.unfreeze()\n",
        "            #P.unfreeze()\n",
        "            #Q.freeze()\n",
        "            #if opt.fix_emb: F.freeze_emb_layer()\n",
        "            #elif epoch>3: F.unfreeze_emb_layer()\n",
        "            #F.unfreeze_emb_layer()\n",
        "\n",
        "            \"\"\" extract features : f_src, f_tgt = F(x_src), F(x_tgt) \"\"\"\n",
        "            DEBUG(\"\"\" calculate loss : loss = Lp(P(f_src); y_src) + λ * (Q(f_src) - Q(f_tgt)) \"\"\")\n",
        "            #metrices = optimizer_FP.call(inputs_src, inputs_tgt, labels_src, labels_tgt=None, _lambda=opt._lambda, supervised=False)\n",
        "            #pred = argmax32(o_src_sent)\n",
        "            #total += len(labels_src)\n",
        "            #correct += np.sum(pred == labels_src)\n",
        "\n",
        "        #log.info('\\n\\nl_src_ad = \\n' + str(l_src_ad))\n",
        "        #log.info('\\n\\nl_tgt_ad = \\n' + str(l_tgt_ad))\n",
        "        #log.info(f'\\n\\n result :\\n' + str(metrices))\n",
        "\n",
        "    log.info('\\nMain iteration done.')\n",
        "    log.info(f' (Q(features_src) < Q(features_tgt)) : {np.sum(Q(features_src) < Q(features_tgt))}')\n",
        "    log.info(f' (Q(features_src) > Q(features_tgt)) : {np.sum(Q(features_src) > Q(features_tgt))}')\n",
        "    log.info(f' Q precision in differentiating src-tgt : {np.sum(Q(features_src) > Q(features_tgt)) / (np.sum(Q(features_src) < Q(features_tgt)) + np.sum(Q(features_src) > Q(features_tgt)))}')\n",
        "    log.info(f' Q accuracy : unknown')\n",
        "    #log.info(f'\\n\\n RESULT :\\n' + str(metrices))\n",
        "\n",
        "# train.py\n",
        "#if __name__ == \"__main__\":\n",
        "#    train(opt)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}