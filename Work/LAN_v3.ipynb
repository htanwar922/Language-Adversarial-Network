{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LAN_copy_v3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JW8PIpkLCrfs","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title mount\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd '/gdrive/My Drive/Colab Notebooks/Work'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HS4dJVSEr-wa","colab_type":"code","cellView":"form","colab":{}},"source":["#@title options.py\n","\n","#!/usr/bin/env ipython\n","\n","import tensorflow as tf\n","\n","import os, sys, logging, argparse\n","from pathlib import Path\n","\n","__file__ = '/gdrive/My Drive/Colab Notebooks/Work/LAN.ipynb'   # uncomment only when in notebook\n","os.chdir(os.path.dirname(__file__))\n","sys.argv = [__file__, '--learning_rate', '0.1', '--Q_learning_rate', '0.1', '--clipvalue', '10', '--epochs', '5', '--n_vecs', '-1', '--train_size_src', '-1', '--train_size_tgt', '-1', '--batch_size', '50000', '--no_F_bn', '--no_P_bn', '--no_Q_bn', '--vector_length', '10']\n","sys.argv += ['--notebook', 'True']\n","parser = argparse.ArgumentParser()\n","\n","#platform arguments\n","parser.add_argument('--notebook', type=bool, default=False)\n","\n","# dataset arguments\n","parser.add_argument('--data_path', default=None)\n","parser.add_argument('--src_lang', default='en')\n","parser.add_argument('--tgt_lang', default='fr')\n","parser.add_argument('--train_size_src', type=int, default=None)        # use all\n","parser.add_argument('--train_size_tgt', type=int, default=None)        # use all\n","parser.add_argument('--num_labels', type=int, default=5+1)            # max reviews rating\n","parser.add_argument('--iterate', action='store_true')                # read through iterations\n","parser.add_argument('--label_dtype', default=tf.int32)\n","\n","# sequences and vocab arguments\n","parser.add_argument('--max_seq_len', type=int, default=100)            # None for no truncate\n","parser.add_argument('--unk_tok', type=str, default='<unk>')\n","parser.add_argument('--bos_tok', type=str, default='<s>')\n","parser.add_argument('--eos_tok', type=str, default='</s>')\n","\n","# training arguments\n","parser.add_argument('--epochs', type=int, default=5)\n","parser.add_argument('--random_seed', type=int, default=1)\n","parser.add_argument('--model_save_file', default='./saved_models/adan')\n","parser.add_argument('--batch_size', type=int, default=10000)\n","parser.add_argument('--buffer_size', type=int, default=40000)\n","parser.add_argument('--learning_rate', type=float, default=0.05)\n","parser.add_argument('--Q_learning_rate', type=float, default=0.05)\n","\n","# bwe arguments\n","parser.add_argument('--emb_filename', default='')\n","parser.add_argument('--n_vecs', type=int, default=-1)\n","parser.add_argument('--random_emb', action='store_true')\n","parser.add_argument('--fix_unk', action='store_true')                # use a fixed <unk> token for all words without pretrained embeddings when building vocab\n","parser.add_argument('--emb_size', type=int, default=300)\n","parser.add_argument('--pre_trained_src_emb_file', type=str, default='bwe/vectors/wiki.multi.en.vec')\n","parser.add_argument('--pre_trained_tgt_emb_file', type=str, default='bwe/vectors/wiki.multi.fr.vec')\n","\n","# Feature Extractor\n","parser.add_argument('--model', default='dan')                        # dan or lstm or cnn\n","parser.add_argument('--fix_emb', action='store_true')\n","parser.add_argument('--vector_length', type=int, default=1)\n","# for LSTM model\n","parser.add_argument('--attn', default='dot')                        # attention mechanism (for LSTM): avg, last, dot\n","parser.add_argument('--bidir_rnn', dest='bidir_rnn', action='store_true', default=True)        # bi-directional LSTM\n","parser.add_argument('--sum_pooling/', dest='avg_pooling', action='store_false')\n","parser.add_argument('--avg_pooling/', dest='avg_pooling', action='store_true')\n","# for CNN model\n","parser.add_argument('--kernel_num', type=int, default=400)\n","parser.add_argument('--kernel_sizes', type=int, nargs='+', default=[3,4,5])\n","\n","# for layers and all models\n","parser.add_argument('--F_layers', type=int, default=1)\n","parser.add_argument('--P_layers', type=int, default=1)\n","parser.add_argument('--Q_layers', type=int, default=1)\n","\n","parser.add_argument('--q_critic', type=int, default=5)    # Q iterations\n","parser.add_argument('--_lambda', type=float, default=0.1)\n","\n","parser.add_argument('--F_bn/', dest='F_bn', action='store_true')\n","parser.add_argument('--no_F_bn/', dest='F_bn', action='store_false')\n","parser.add_argument('--P_bn/', dest='P_bn', action='store_true', default=True)\n","parser.add_argument('--no_P_bn/', dest='P_bn', action='store_false')\n","parser.add_argument('--Q_bn/', dest='Q_bn', action='store_true', default=True)\n","parser.add_argument('--no_Q_bn/', dest='Q_bn', action='store_false')\n","\n","parser.add_argument('--hidden_size', type=int, default=900)\n","parser.add_argument('--dropout', type=float, default=0)\n","parser.add_argument('--activation', type=str, default='linear')\n","\n","parser.add_argument('--clip_Q', type=bool, default=False)\n","parser.add_argument('--clipvalue', type=float, default=0.01)\n","parser.add_argument('--clip_lim_FP', type=float, default=None)\n","\n","parser.add_argument('--device', type=str, default='cuda')\n","parser.add_argument('--debug/', dest='debug', action='store_true')\n","\n","opt = parser.parse_args()\n","\n","if not tf.config.list_physical_devices('GPU'):\n","    opt.device = 'CPU'\n","\n","logging.basicConfig(stream=sys.stderr, level=logging.DEBUG if opt.debug else logging.INFO)\n","log = logging.getLogger(__name__)\n","import errno\n","filename = Path(opt.model_save_file) / 'log.txt'\n","if not os.path.exists(os.path.dirname(filename)):\n","    try: os.makedirs(os.path.dirname(filename))\n","    except OSError as exc:\n","        if exc.errno != errno.EEXIST: raise # Guard against race condition\n","with open(filename, \"w\") as f: pass\n","fh = logging.FileHandler(Path(opt.model_save_file) / 'log.txt')\n","log.addHandler(fh)\n","\n","if __name__ == \"__main__\":\n","    print(\"src_embeddings: \", opt.pre_trained_src_emb_file)\n","    print(\"tgt_embeddings: \", opt.pre_trained_tgt_emb_file)\n","    print(\"debugging: \", opt.debug)\n","    print(opt.n_vecs)\n","    log.info('start...')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mnLNtzFHf4Qu","colab_type":"code","cellView":"form","colab":{},"executionInfo":{"status":"ok","timestamp":1593572777852,"user_tz":-330,"elapsed":1658,"user":{"displayName":"Himanshu Tanwar","photoUrl":"","userId":"00569403243595938133"}}},"source":["#@title vocab.py\n","\n","#!/usr/bin/env ipython\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models, preprocessing\n","\n","import numpy as np\n","import os, io, sys, logging\n","from pathlib import Path\n","from tqdm import tqdm, trange\n","os.chdir(os.path.dirname(__file__))\n","\n","#from options import *\n","\n","class Vocab:\n","    \"\"\"\n","attributes: (self)\n","        vocab_size\n","        emb_size\n","        embeddings \n","        w2vvocab = {word : idx}\n","        v2wvocab = [idx => emb_vector]\n","        pt_w2vvocab = {word : idx}\n","        pt_v2wvocab = [idx => emb_vector]\n","        unk_tok\n","        unk_idx\n","        eos_tok\n","        eos_idx\n","\n","methods: (self)\n","        __init__(self, pre_train_infile)\n","        base_form(word)\n","        new_rand_emb(self)\n","        init_embed_layer(self)\n","        add_word(self, word)\n","        clear_pretrained_vectors(self)\n","        lookup(self, word)\n","        get_word(self, i)\n","\n","    \"\"\"\n","    \n","    def __init__(self, pre_train_infile = None, vecs = opt.n_vecs):\n","        \"\"\"\n","        load pre-trained words - embedding vectors in pt_***vocabs and initialise ***vocabs\n","\n","        \"\"\"\n","        self.vocab_size = 0\n","        self.emb_size = opt.emb_size\n","        self.embeddings = []\n","        self.w2vvocab = {}\n","        self.v2wvocab = []\n","        \n","        self.pt_v2wvocab = []\n","        self.pt_w2vvocab = {}\n","        self.cnt = 0\n","        \n","        # add <unk>\n","        self.unk_tok = opt.unk_tok\n","        self.add_word(self.unk_tok)\n","        opt.unk_idx = self.unk_idx = self.w2vvocab[self.unk_tok]\n","        self.embeddings[self.unk_idx][:] = 0\n","        # add BOS token\n","        self.bos_tok = opt.bos_tok\n","        self.add_word(self.bos_tok)\n","        opt.bos_idx = self.bos_idx = self.unk_idx #self.w2vvocab[self.bos_tok]\n","        #self.embeddings[self.bos_idx][:] = 0\n","        # add EOS token\n","        self.eos_tok = opt.eos_tok\n","        self.add_word(self.eos_tok)\n","        opt.eos_idx = self.eos_idx = self.unk_idx #self.w2vvocab[self.eos_tok]\n","        #self.embeddings[self.eos_idx][:] = 1    # 0\n","        log.info(\"vocab initializing...done.\")\n","        # add pre trained embeddings\n","        self.add_pre_trained_emb(pre_train_infile, vecs)\n","\n","    def add_pre_trained_emb(self, pre_train_infile = None, vecs = opt.n_vecs):\n","        # load pretrained embedings\n","        if(pre_train_infile is None): raise Exception('file not specified...')\n","        if(os.path.isfile(pre_train_infile)):\n","            log.info('reading pre-trained embeddings from ' + pre_train_infile + '...')\n","            with io.open(Path(pre_train_infile), 'r', encoding='utf-8') as infile:\n","                first_line = infile.readline().split()\n","                assert len(first_line) == 2\n","                n_vecs, emb_dim = map(int, first_line)    # first line has total number of vectors and embedding dimensions\n","                assert emb_dim == self.emb_size\n","                self.emb_size = emb_dim\n","                if vecs is not None and vecs > 0: n_vecs = min(n_vecs, vecs)\n","                if not hasattr(self, 'pretrained'):    self.pretrained = np.empty(shape=(n_vecs, emb_dim), dtype=np.float)\n","                else: self.pretrained = np.append(self.pretrained, np.empty(shape=(n_vecs, emb_dim), dtype=np.float), axis=0)\n","                for _ in trange(n_vecs):\n","                    line = infile.readline()\n","                    if not line: break\n","                    parts = line.rstrip().split(' ')\n","                    word = parts[0]\n","                    #if word in self.pt_v2wvocab: continue        # no need to check if assumed no repetition mistake\n","                    # add to vocabs\n","                    self.pt_v2wvocab.append(word)\n","                    self.pt_w2vvocab[word] = self.cnt\n","                    vector = [float(x) for x in parts[1:]]\n","                    self.pretrained[self.cnt] = vector\n","                    self.cnt += 1\n","            log.info(\"embedding vectors imported...\")\n","        else:\n","            raise FileNotFoundError(log.info(\"pre_train_file \", Path(pre_train_infile), \" does not exist...\"))\n","\n","    def base_form(self, word):\n","        \"\"\"\n","        return stripped and lowercased word\n","        \"\"\"\n","        return word.strip().lower()\n","\n","\n","    def new_rand_emb(self):\n","        \"\"\"\n","        return a normal random emb_vector\n","        \"\"\"\n","        vec = np.random.normal(-1, 1, size=self.emb_size)\n","        vec /= sum(x*x for x in vec) ** .5\n","        return vec\n","\n","\n","    def add_word(self, word, vec=None):\n","        \"\"\"\n","        add new word to the ***vocab. \\nUse this to only add new words or used words from pt_***vocabs to ***vocabs.\n","        \"\"\"\n","        word = self.base_form(word=word)\n","        if word not in self.w2vvocab:\n","            if not opt.random_emb and hasattr(self, 'pt_w2vvocab'):\n","                if opt.fix_unk and word not in self.pt_w2vvocab:\n","                    # use fixed unk token, do not update vocab\n","                    return\n","                if word in self.pt_w2vvocab:\n","                    vector = self.pretrained[self.pt_w2vvocab[word]].copy()\n","                else:\n","                    vector = self.new_rand_emb() if vec is None else vec\n","            else:\n","                vector = self.new_rand_emb() if vec is None else vec\n","            self.v2wvocab.append(word)\n","            self.w2vvocab[word] = self.vocab_size\n","            self.embeddings.append(vector)\n","            self.vocab_size += 1\n","        return self.w2vvocab[word]\n","\n","\n","    def lookup(self, word):\n","        \"\"\"\n","        return value of word (word_idx) from w2vvocab\n","        \"\"\"\n","        word = Vocab.base_form(word)\n","        if word in self.w2vvocab:\n","            return self.w2vvocab[word]\n","        return self.unk_idx\n","\n","\n","    def get_word(self, i):\n","        \"\"\"\n","        return emb_vector at index i\n","        \"\"\"\n","        return self.v2wvocab[i]\n","    \n","\n","    def hash_fit_on_text(self, line):\n","        \"\"\"\n","        fit on text (sentence) with tf.keras.preprocessing.text.hashing_trick\\n(NOT TESTED)\n","        \"\"\"\n","        return preprocessing.text.hashing_trick(line, n=self.vocab_size, hash_function=self.lookup, filters='')\n","\n","\n","    def text_to_sequence(self, line, update_vocab=True):\n","        \"\"\"\n","        convert text (line) to sequence\n","        \"\"\"\n","        return [self.add_word(w) for w in line.strip().split()] if update_vocab else\\\n","            [self.lookup(w) for w in line.strip().split()]\n","\n","\n","    def text_list_to_sequence(self, text_list, update_vocab=True):\n","        \"\"\"\n","        convert text (word list) to sequence\n","        \"\"\"\n","        return [self.add_word(w) for w in text_list] if update_vocab else\\\n","            [self.lookup(w) for w in text_list]\n","\n","\n","    def fit_on_text(self, line_list):    # tf.keras.preprocessing.text.hashing_trick\n","        \"\"\"\n","        add new text (sentence) to the vocabularies\n","        \"\"\"\n","        return [[self.add_word(w) for w in line.strip().split()] for line in line_list]\n","\n","\n","    def fit_on_text_list(self, texts_list):    # tf.keras.preprocessing.text.hashing_trick\n","        \"\"\"\n","        add new text (sentence) to the vocabularies\n","        \"\"\"\n","        return [[self.add_word(w) for w in line] for line in texts_list]\n","\n","\n","    def pad_text_list(self, text_list, max_len=opt.max_seq_len, pad='pre', truncate='post', add_eos_tok=False):\n","        \"\"\"\n","        pad single text (words) list\n","        \"\"\"\n","        if add_eos_tok: max_len -= 1\n","        text_list = text_list[:max_len]\n","        text_list = ['<s>' for _ in range(max_len - len(text_list))] + text_list\n","        if add_eos_tok: text_list += ['</s>']\n","        return text_list\n","\n","\n","    def pad_sequences(self, dataset, max_len=opt.max_seq_len, pad='pre', truncate='post', add_eos=False):\n","        \"\"\"\n","        pad list of sequences (sequence : list of int) with keras.preprocessing.sequence.pad_sequences\n","        \"\"\"\n","        if add_eos: max_len -= 1\n","        seq_list, lengths, stars = zip(*dataset)\n","        seq_list = self.fit_on_text_list(seq_list)\n","        seq_list = preprocessing.sequence.pad_sequences(tqdm(seq_list), maxlen=max_len, truncating='post', value=opt.bos_idx)\n","        if add_eos: seq_list = preprocessing.sequence.pad_sequences(tqdm(seq_list), maxlen=max_len+1, padding='post', value=opt.eos_idx)\n","        return tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(seq_list, name='seq'), tf.convert_to_tensor(lengths, name='len'), tf.convert_to_tensor(stars, name='label')))\n","\n","    def clear_pretrained_vectors(self):\n","        \"\"\"\n","        clear the pretrained vectors and pt_***vocab\n","        \"\"\"\n","        if hasattr(self, 'pretrained'): del self.pretrained\n","        if hasattr(self, 'pt_w2vvocab'): del self.pt_w2vvocab\n","        if hasattr(self, 'pt_v2wvocab'): del self.pt_v2wvocab\n","    \n","\n","    def init_embed_layer(self, clear_pt=True):\n","        \"\"\"\n","        clear pretrained vectors and return an embedding layer initialized with self.embeddings\n","        \"\"\"\n","        if clear_pt: self.clear_pretrained_vectors()\n","        emb_layer = layers.Embedding(input_dim=self.vocab_size, output_dim=self.emb_size, input_length=opt.max_seq_len, name='vocab_embedding')\n","        emb_layer.build(input_shape=(None, self.vocab_size, self.emb_size))\n","        emb_layer.set_weights(np.array([self.embeddings], dtype=float))\n","        emb_layer.trainable = False\n","        assert emb_layer.weights[0].shape[0] == self.vocab_size, \"layer weights len not equal to vocab size in layer \" + emb_layer.name\n","        return emb_layer\n","\n","\n","if __name__ == \"__main__\" and not opt.notebook:\n","    \"\"\"\n","    run as main\n","    \"\"\"\n","    print(opt.pre_trained_src_emb_file)\n","    vocab = Vocab(opt.pre_trained_src_emb_file, vecs=5000)\n","    vocab.add_pre_trained_emb(opt.pre_trained_tgt_emb_file, vecs=5000)\n","    vocab.add_word('the')\n","    vocab.add_word('of')\n","    vocab.add_word('this')\n","    emb_layer = vocab.init_embed_layer(clear_pt=False)\n","    print(emb_layer.variables)\n","\n","# imp link https://www.tensorflow.org/tfx/tutorials/transform/census , https://www.tensorflow.org/api_docs/python/tf/numpy_function , https://www.tensorflow.org/api_docs/python/tf/py_function"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKHVCROGjkdW","colab_type":"code","cellView":"form","colab":{},"executionInfo":{"status":"ok","timestamp":1593572778476,"user_tz":-330,"elapsed":1424,"user":{"displayName":"Himanshu Tanwar","photoUrl":"","userId":"00569403243595938133"}}},"source":["#@title utils.py\n","\n","#!/usr/bin/env ipython\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models, preprocessing\n","\n","import pdb\n","import numpy as np\n","\n","#from options import *\n","\n","DEBUG = lambda x: print(x)\n","\n","def freeze(net):\n","    net.trainable = False\n","\n","def unfreeze(net):\n","    net.trainable = True\n","\n","def get_lines(infile, encoding='utf-8'):\n","    if os.sep != '\\\\': return int(subprocess.Popen(f\"wc -l \\\"{str(Path(infile))}\\\"\", shell=True, stdout=subprocess.PIPE).stdout.read().split()[0])\n","    with io.open(Path(infile), encoding=encoding) as foo:\n","        lines = sum(1 for line in foo)  #os.path.getsize(infile)\n","    return lines\n","\n","def argmax32(arr, axis=-1, dtype=opt.label_dtype):\n","    return tf.cast(np.argmax(arr, axis=-1), dtype=dtype)\n","\n","def pad(x : list, y : list, eos_idx : int, sort:bool=False):\n","    #inputs, lengths = zip(*x)\n","    inputs = x\n","    lengths = [len(l) for l in x]\n","    max_len = max(lengths)\n","    # pad sequences\n","    padded_inputs = tf.fill((len(inputs), max_len), eos_idx, dtype=tf.int64)\n","    for i, row in enumerate(inputs):\n","        assert eos_idx not in row, f'EOS in sequence {row}'\n","        padded_inputs[i][:len(row)] = tf.convert_to_tensor(row, dtype=tf.int64)\n","    lengths = tf.convert_to_tensor(lengths, dtype=tf.int64)\n","    y = tf.reshape(tf.convert_to_tensor(y, dtype=tf.int64), -1)\n","    if sort:\n","        # sort by length\n","        sorted_lengths = lengths.sort(axis=0, direction='DESCENDING')\n","        sorting_idx = keras.backend.eval(sorted_lengths)\n","        padded_inputs = tf.gather(params=padded_inputs, indices=sorting_idx, axis=0)\n","        y = tf.gather(params=y, indices=sorting_idx, axis=0)\n","        return (padded_inputs, sorted_lengths), y\n","    else:\n","        return (padded_inputs, lengths), y\n","\n","\n","def my_collate(batch : list, sort : bool):\n","    x, y = zip(*batch)\n","    with tf.device(opt.device):\n","        x, y = pad(x, y, opt.eos_idx, sort)\n","    return (x, y)\n","\n","\n","def sorted_collate(batch):\n","    return my_collate(batch, sort=True)\n","\n","def unsorted_collate(batch):\n","    return my_collate(batch, sort=False)\n","\n","\n","if __name__ == \"__main__\" and not opt.notebook:\n","    print(get_lines(\"bwe/vectors/wiki.multi.en.vec\"))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"-VdgqJq_iCsg","colab_type":"code","cellView":"form","colab":{},"executionInfo":{"status":"ok","timestamp":1593572780117,"user_tz":-330,"elapsed":1423,"user":{"displayName":"Himanshu Tanwar","photoUrl":"","userId":"00569403243595938133"}}},"source":["#@title data.py\n","\n","#!/usr/bin/env ipython\n","\n","import tensorflow as tf\n","\n","import sys, os, io, json, subprocess\n","from pathlib import Path\n","from tqdm import tqdm, trange\n","os.chdir(os.path.dirname(__file__))\n","\n","#from options import opt\n","#from vocab import *\n","\n","label_dtype = opt.label_dtype\n","\n","def decode_json(infile, lines=None, reviews_data=None, max_seq_len=None):\n","    assert os.path.isfile(Path(infile)), str(os.getcwd() / infile) + \" doesn't exist, extract_data first\"\n","    if lines is None or lines < 0: lines = get_lines(infile)\n","    log.info(f'Reading {lines} lines from {infile}')\n","    with io.open(Path(infile), 'r', encoding='utf-8') as infile:\n","        if reviews_data == 'Amazon reviews':\n","            ret = []\n","            max_stars = 0\n","            for _ in trange(lines): # line in infile\n","                dic = json.loads(infile.readline())\n","                line = str(dic[\"review_title\"] + dic[\"review_body\"]).strip().split()[:max_seq_len]\n","                ret += [[line, len(line), tf.cast(int(dic['stars']), dtype=opt.label_dtype)]]\n","                max_stars = max(max_stars, int(dic['stars']))\n","            return ret, max_stars\n","        return [json.loads(line) for line in tqdm(infile.read().strip().split('\\n')[:lines])]\n","\n","\n","def decode_json_iterate(infile, lines=None, reviews_data=None, max_seq_len=None):\n","    assert os.path.isfile(Path(infile)), str(os.getcwd() / infile) + \" doesn't exist, extract_data first\"\n","    with tqdm(total=get_lines(infile) if not lines else lines) as pbar:\n","        with io.open(Path(infile), 'r', encoding='utf-8') as infile:\n","            if lines is not None and lines > 0: z = zip(trange(1, lines+1), infile)\n","            else: z = enumerate(infile)\n","            for num, line in z:\n","                if not line or lines is not None and num > lines: break;\n","                dic = json.loads(line)\n","                pbar.update(len(line) if not lines else 1)\n","                yield dic if reviews_data != 'Amazon reviews' \\\n","                    else [str(dic[\"review_title\"] + dic[\"review_body\"]).strip().split()[:max_seq_len], len(str(dic[\"review_title\"] + dic[\"review_body\"]).strip().split()[:max_seq_len]), tf.cast(int(dic[\"stars\"]), dtype=opt.label_dtype)]\n","\n","\n","class AmazonReviews:\n","    \"\"\"\n","    get Amazon reviews data from the extracted data => review title + review body + eos_tok : stars\n","parameters:\n","    path : str => path to 'Amazon reviews' directory with '/' as separator\n","    eos_tok : str\n","    max_seq_len : int\n","    \"\"\"\n","    def __init__(self, path:str=None, eos_tok=opt.eos_tok, max_seq_len=opt.max_seq_len, star_rating=5):\n","        super(AmazonReviews, self).__init__()\n","        self.path = Path('Amazon reviews') if not path else Path(path)\n","        self.dats = {}\n","        self.dats['train'] = self.path / 'train'\n","        self.dats['dev'] = self.path / 'dev'\n","        self.dats['test'] = self.path / 'test'\n","        self.eos_tok = eos_tok\n","        self.max_seq_len = max_seq_len\n","        opt.labels = self.star_rating = star_rating\n","\n","\n","    def load_data(self, lang, dat, lines=-1):\n","        \"\"\"\n","        load all data in one go\n","parametrs:\n","        lang : str => de, en, es, fr, ja, zh\n","        dat : str => train, dev, test\n","        lines : int\n","return:\n","        tuple of\n","            dataset of reviews (split) and their star ratings\n","            max_seq_length\n","        \"\"\"\n","        infile = self.dats[dat] / str('dataset_' + lang + '_' + dat + '.json')\n","        data, self.star_rating = decode_json(infile, lines=lines, reviews_data='Amazon reviews', max_seq_len=self.max_seq_len)\n","        return data\n","\n","\n","    def load_data_generator(self, lang, dat, lines=-1):\n","        \"\"\"\n","        iterate over the data file line by line (for less RAM devices like this one - not completely implemented)\n","parametrs:\n","        lang : str => de, en, es, fr, ja, zh\n","        dat : str => train, dev, test\n","        lines : int\n","yield:\n","        generator witch generates a list of one review and its star rating at a time\n","        \"\"\"\n","        infile = self.dats[dat] / str('dataset_' + lang + '_' + dat + '.json')\n","        return decode_json_iterate(infile, lines=lines, reviews_data='Amazon reviews', max_seq_len=self.max_seq_len)\n","\n","\n","if __name__ == \"__main__\" and not opt.notebook:\n","    infile = 'Amazon reviews/test/dataset_en_test.json'\n","    #assert os.path.isfile(infile), str(os.getcwd() / infile) + \" doesn't exist\"\n","    #for x in decode_json_iterate(infile, 30): print(x)\n","    #print(decode_json(infile, 30))\n","    #vocab = Vocab(opt.pre_trained_src_emb_file)\n","    rev = AmazonReviews()\n","    data = rev.load_data(lang='en', dat='train')\n","    data = vocab.pad_sequences(data)\n","    print('\\n', data)\n","    for dat in data.take(3):\n","        print(dat)\n","    #sequence_input = keras.Input(input_shape=(opt.max_seq_len,), dtype='int32')(np.array([x for x, y in data.as_numpy_iterator()], dtype='int32'))\n","    #emb_layer = vocab.init_embed_layer()\n","    #emb_layer(sequence_input)\n","    #print(emb_layer(tf.convert_to_tensor([x for x, y in data.as_numpy_iterator()], dtype='int32')))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxWU46Jbkyew","colab_type":"code","cellView":"form","colab":{}},"source":["#@title train_data.py\n","\n","#!/usr/bin/env ipython\n","\n","#import torch\n","#import torch.nn as nn\n","#import torch.nn.functional as functional\n","#import torch.optim as optim\n","#from torch.utils.data import DataLoader\n","#from torchnet.meter import ConfusionMeter\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras import optimizers, losses\n","import json\n","\n","import os, random, sys, logging, argparse\n","from tqdm import tqdm\n","from pathlib import Path\n","os.chdir(os.path.dirname(__file__))\n","\n","#from options import *\n","#from data import *\n","#from vocab import *\n","#from utils import *\n","#from models import *\n","\n","#tf.logging.set_verbosity(tf.logging.INFO)\n","#tf.logging.set_verbosity(True)\n","\n","#random.seed(opt.random_seed)\n","#torch.manual_seed(opt.random_seed)\n","\n","# save logs\n","if not os.path.exists(opt.model_save_file): os.makedirs(opt.model_save_file)\n","logging.basicConfig(stream=sys.stderr, level=logging.DEBUG if opt.debug else logging.INFO)\n","log = logging.getLogger(__name__)\n","fh = logging.FileHandler(os.path.join(opt.model_save_file, 'log.txt'))\n","log.addHandler(fh)\n","\n","# output options\n","log.info('Training ADAN with options:')\n","log.info(opt)\n","\n","def get_train_data(opt):\n","    #opt.n_vecs = 20000; opt.train_size_src = -1; opt.train_size_tgt = -1\n","    # vocab\n","    log.info(f'Loading Embeddings...')\n","    vocab = Vocab(opt.pre_trained_src_emb_file, opt.n_vecs)\n","    vocab.add_pre_trained_emb(opt.pre_trained_tgt_emb_file, opt.n_vecs)\n","    log.info(f'Done.')\n","\n","    # datasets\n","    length = {}\n","\n","    # src_lang datasets\n","    log.info(f'Loading src datasets...')\n","    reviews_src_obj = AmazonReviews(path=opt.data_path, max_seq_len=opt.max_seq_len)\n","    train_src = reviews_src_obj.load_data(lang=opt.src_lang, dat='train', lines=opt.train_size_src); length['train_src'] = len(train_src)\n","    dev_src = reviews_src_obj.load_data(lang=opt.src_lang, dat='dev', lines=-1); length['dev_src'] = len(dev_src)\n","    test_src = reviews_src_obj.load_data(lang=opt.src_lang, dat='test', lines=-1); length['test_src'] = len(test_src)\n","    log.info('Done loading src datasets.')\n","\n","    # tgt_lang datasets\n","    log.info(f'Loading tgt datasets...')\n","    reviews_tgt_obj = AmazonReviews(path=opt.data_path, max_seq_len=opt.max_seq_len)\n","    train_tgt = reviews_tgt_obj.load_data(lang=opt.tgt_lang, dat='train', lines=opt.train_size_tgt); length['train_tgt'] = len(train_tgt)\n","    dev_tgt = reviews_tgt_obj.load_data(lang=opt.tgt_lang, dat='dev', lines=-1); length['dev_tgt'] = len(dev_tgt)\n","    test_tgt = reviews_tgt_obj.load_data(lang=opt.tgt_lang, dat='test', lines=-1); length['test_tgt'] = len(test_tgt)\n","    \n","    log.info('Done loading tgt datasets.')\n","\n","    #opt.num_labels = max(reviews_src_obj.star_rating, reviews_tgt_obj.star_rating)\n","    if opt.max_seq_len < 0 or not opt.max_seq_len:\n","        maxlen_src, maxlen_tgt = max(list(len(x) for x in train_src)), max(list(len(x) for x in train_tgt))\n","        opt.max_seq_len = max(maxlen_src, maxlen_tgt)\n","    del reviews_src_obj, reviews_tgt_obj\n","\n","    # pad src datasets (-> Dataset)\n","    log.info('Padding src datasets...')\n","    train_src = vocab.pad_sequences(train_src, max_len=opt.max_seq_len)\n","    dev_src = vocab.pad_sequences(dev_src, max_len=opt.max_seq_len)\n","    test_src = vocab.pad_sequences(test_src, max_len=opt.max_seq_len)\n","    log.info('Done padding src datasets...')\n","\n","    # pad tgt datasets (-> Dataset)\n","    log.info('Padding tgt datasets...')\n","    train_tgt = vocab.pad_sequences(train_tgt, max_len=opt.max_seq_len)\n","    dev_tgt = vocab.pad_sequences(dev_tgt, max_len=opt.max_seq_len)\n","    test_tgt = vocab.pad_sequences(test_tgt, max_len=opt.max_seq_len)\n","    log.info('Done padding tgt datasets...')\n","\n","    # dataset loaders\n","    log.info('Shuffling and batching...')\n","    train_src = train_src.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size).shuffle(length['train_src']//opt.batch_size).shuffle(length['train_src']//opt.batch_size).shuffle(length['train_src']//opt.batch_size)\n","    train_tgt = train_tgt.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size).shuffle(length['train_tgt']//opt.batch_size).shuffle(length['train_tgt']//opt.batch_size).shuffle(length['train_tgt']//opt.batch_size)\n","    with tf.device('CPU'):\n","        train_src_Q = tf.identity(train_src)\n","        train_tgt_Q = tf.identity(train_src)\n","    train_src_Q_iter = iter(train_src_Q)\n","    train_tgt_Q_iter = iter(train_tgt_Q)\n","    \n","    dev_src = dev_src.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size)\n","    dev_tgt = dev_tgt.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size)\n","    \n","    test_src = test_src.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size)\n","    test_tgt = test_tgt.shuffle(buffer_size=opt.buffer_size, reshuffle_each_iteration=True).batch(opt.batch_size)\n","    log.info('Done shuffling and batching.')\n","\n","    return vocab, train_src, dev_src, test_src, train_tgt, dev_tgt, test_tgt, train_src_Q, train_tgt_Q, train_src_Q_iter, train_tgt_Q_iter, length\n","\n","if __name__ == \"__main__\" and opt.notebook:\n","    # clear dumps\n","    tf.keras.backend.clear_session()\n","    tf.keras.backend.set_learning_phase(0)\n","    print(tf.keras.backend.learning_phase())\n","    vocab, train_src, dev_src, test_src, train_tgt, dev_tgt, test_tgt, train_src_Q, train_tgt_Q, train_src_Q_iter, train_tgt_Q_iter, length = get_train_data(opt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39GTfkexjyur","colab_type":"code","cellView":"form","colab":{},"executionInfo":{"status":"ok","timestamp":1593572927683,"user_tz":-330,"elapsed":145357,"user":{"displayName":"Himanshu Tanwar","photoUrl":"","userId":"00569403243595938133"}}},"source":["#@title layers.py\n","\n","#!/usr/bin/env ipython\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models\n","\n","import numpy as np\n","\n","#from options import *\n","#from vocab import *\n","#from data import *\n","#from utils import *\n","\n","class Averaging(layers.Layer):\n","    def __init__(self, toks=None, vector_length=1, **kwargs):\n","        super(Averaging, self).__init__(**kwargs)\n","        self.toks = [opt.unk_idx, opt.bos_idx, opt.eos_idx] if toks is None else toks\n","        self.vl = vector_length\n","\n","    def call(self, embeddings, lengths):\n","        self.W = tf.cast(tf.reduce_sum(embeddings, axis=1), dtype=tf.float32)            # (BSZ, EMBDIM)\n","        return self.W/tf.cast(tf.reshape(lengths, (-1, 1)), dtype=tf.float32) * self.vl   # (BSZ, EMBDIM)    #list(lengths.numpy())\n","    \n","if __name__ == \"__main__\" and not opt.notebook:\n","    infile = 'Amazon reviews/test/dataset_en_test.json'\n","    vocab = Vocab(opt.pre_trained_src_emb_file, vecs=10000)\n","    rev = AmazonReviews()\n","    data = rev.load_data(lang='en', dat='train', lines=100)\n","    data = vocab.pad_sequences(data)\n","    emb_layer = vocab.init_embed_layer()\n","    avg = Averaging()\n","    for x, l, y in data.take(10):\n","        print(avg(emb_layer(x), l))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0MthXomstTh","colab_type":"code","colab":{},"cellView":"form","executionInfo":{"status":"ok","timestamp":1593572930441,"user_tz":-330,"elapsed":147529,"user":{"displayName":"Himanshu Tanwar","photoUrl":"","userId":"00569403243595938133"}}},"source":["#@title models.py\n","\n","#!/usr/bin/env ipython\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models, optimizers, preprocessing, losses\n","from tensorflow.keras import backend as K\n","from tensorflow.data import Dataset\n","\n","import numpy as np\n","import os, io\n","from pathlib import Path\n","os.chdir(os.path.dirname(__file__))\n","\n","#from options import opt\n","#from layers import *\n","\n","def absexp_1(x):\n","    return tf.clip_by_value(tf.math.expm1(tf.cast(tf.abs(x), dtype=tf.float64)) * tf.sign(x), -1.7e+308, 1.7e+308)\n","\n","def scce(y_true, y_pred):\n","    return losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n","\n","def hinge(ll_lang, ll_pred):\n","    return losses.Hinge()(ll_true, ll_pred)\n","\n","def total_loss(y_ll_true, y_ll_pred):\n","    y_true, ll_true = zip(*y_ll_true)\n","    y_pred, ll_pred = zip(*y_ll_pred)\n","    return scce(y_true, y_pred) + opt._lambda * hinge(ll_true, ll_pred)\n","\n","opt.num_labels = 6\n","opt._lambda = 1.\n","opt.F_layers = 2\n","opt.P_layers = 2\n","opt.Q_layers = 2\n","opt.F_activation = 'relu'\n","opt.P_activation = 'relu'\n","opt.Q_activation = 'relu'\n","\n","num_layers, hidden_size, dropout, batch_norm, activation = opt.F_layers, opt.hidden_size, opt.dropout, opt.F_bn, opt.F_activation\n","F = keras.Sequential()\n","for i in range(num_layers):\n","    if dropout > 0: F.add(layers.Dropout(rate=dropout, name=f'Dropout_{i}'))\n","    if i == 0: F.add(layers.Dense(units=hidden_size, input_shape=(vocab.emb_size,), activation=activation, name=f'DenseAbsExp_{i}'))\n","    else: F.add(layers.Dense(units=hidden_size, input_shape=(hidden_size,), activation=activation, name=f'DenseAbsExp_{i}'))\n","    if batch_norm: F.add(layers.BatchNormalization(input_shape=(hidden_size,), name=f'BatchNorm_{i}'))    # same shape as input    # use training=False when making inference from model (model.predict, model.evaluate?)\n","#F.add(layers.LeakyReLU(alpha=0.3))\n","#F.add(layers.ReLU())\n","F.add(layers.Dense(units=hidden_size, input_shape=(hidden_size,), activation=activation, name=f'DenseAbsExpFinal_{i}'))\n","\n","num_layers, hidden_size, output_size, dropout, batch_norm, activation = opt.P_layers, opt.hidden_size, opt.num_labels, opt.dropout, opt.P_bn, opt.P_activation\n","P = models.Sequential()\n","P.add(keras.Input((opt.hidden_size,)))\n","for i in range(num_layers):\n","    if dropout > 0: P.add(layers.Dropout(rate=dropout))\n","    P.add(layers.Dense(units=hidden_size, input_shape=(hidden_size,), activation=opt.activation, name=f'DenseAbsExp_{i}'))\n","    if batch_norm: P.add(layers.BatchNormalization())\n","    #P.add(layers.ReLU())\n","#P.add(layers.Dense(units=output_size, input_shape=(hidden_size,), activation='tanh'))\n","P.add(layers.Dense(units=output_size, input_shape=(hidden_size,), activation='softmax', name=f'DenseSoftmax_{i}'))\n","\n","num_layers, hidden_size, dropout, batch_norm, activation = opt.Q_layers, opt.hidden_size, opt.dropout, opt.Q_bn, opt.Q_activation\n","Q = keras.Sequential()\n","for i in range(num_layers):\n","    if dropout > 0: Q.add(layers.Dropout(rate=dropout, name=f'Dropout_{i}'))\n","    Q.add(layers.Dense(units=hidden_size, input_shape=(hidden_size,), activation=activation, name=f'DenseAbsExp_{i}'))\n","    if batch_norm: Q.add(layers.BatchNormalization(input_shape=(hidden_size,), name=f'BathcNorm_{i}'))\n","Q.add(layers.Dense(units=hidden_size, input_shape=(hidden_size,), activation=activation, name=f'DenseAbsExpFinal_{i}'))\n","Q.add(layers.Dense(units=1, input_shape=(hidden_size,), activation='tanh', name=f'DenseTanh'))\n","#Q.add(layers.Softmax())\n","\n","E = vocab.init_embed_layer()\n","A = Averaging(toks=[vocab.unk_idx, vocab.bos_idx, vocab.eos_idx], vector_length=opt.vector_length)\n","\n","shape = (opt.max_seq_len,)\n","inputs, lengths = keras.Input(shape), keras.Input(())\n","\n","embeddings = E(inputs)\n","outputs_EA = A(embeddings, lengths)\n","EA = keras.Model(inputs=[inputs, lengths], outputs=outputs_EA)\n","\n","outputs_EAF = F(outputs_EA)\n","EAF = keras.Model(inputs=[inputs, lengths], outputs=outputs_EAF, name=\"FeatureExtractor_AE\")\n","\n","outputs_EAFP = P(outputs_EAF)\n","EAFP = keras.Model(inputs=[inputs, lengths], outputs=outputs_EAFP, name=\"SemanticClassifier_FAE\")\n","\n","outputs_EAFQ = Q(outputs_EAF)\n","EAFQ = keras.Model(inputs=[inputs, lengths], outputs=outputs_EAFQ, name=\"LanguageDetector_FAE\")\n","\n","LAN = keras.Model(inputs=[inputs, lengths], outputs=[outputs_EAFP, outputs_EAFQ], name=\"LAN\")"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"mnYOniDE1Jsk","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title train.py : setup\n","TRAIN1 = True\n","if TRAIN1:\n","    log.info('Checking outputs and initializing...')\n","    E.trainable=False\n","    EA.trainable=False\n","    for (inputs, lengths, labels) in train_src.take(1).take(1):\n","        print(inputs)\n","        print(LAN([inputs, lengths]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O7MuBjtvD1_e","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Training statistics : learning_rates\n","opt.learning_rate, opt.Q_learning_rate = 1e-2, 1e-4  # 0.001 is default\n","opt.learning_rate, opt.Q_learning_rate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxjDR8LHgCAy","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Setting the embeddings non-trainable\n","E.trainable = True\n","EA.trainable = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Tdzl8nxcZLw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"cellView":"form","executionInfo":{"status":"ok","timestamp":1593574535679,"user_tz":-330,"elapsed":1640,"user":{"displayName":"Himanshu Tanwar","photoUrl":"","userId":"00569403243595938133"}},"outputId":"f1315b0e-0a66-45f4-d117-b642ca81b639"},"source":["#@title Trainable layers for EAFP training with fixed embeddings\n","[x.name for x in EAFP.trainable_variables]"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['DenseAbsExp_0/kernel:0',\n"," 'DenseAbsExp_0/bias:0',\n"," 'DenseAbsExp_1/kernel:0',\n"," 'DenseAbsExp_1/bias:0',\n"," 'DenseAbsExpFinal_1/kernel:0',\n"," 'DenseAbsExpFinal_1/bias:0',\n"," 'DenseAbsExp_0_1/kernel:0',\n"," 'DenseAbsExp_0_1/bias:0',\n"," 'DenseAbsExp_1_1/kernel:0',\n"," 'DenseAbsExp_1_1/bias:0',\n"," 'DenseSoftmax_1/kernel:0',\n"," 'DenseSoftmax_1/bias:0']"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"vWwpWjduBfBt","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title train.py : Training F and P : sparse categorical\n","EAFP.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","epochs = 5\n","if TRAIN1:\n","    log.info('Training EAFP model with src data with fixed embeddings...')\n","    for epoch in trange(epochs):\n","        batch_no = 0\n","        for (inputs, lengths, labels) in train_src:\n","            log.info(f\"Training on batch no : {batch_no}\"); batch_no += 1\n","            print(scce(labels, EAFP.predict([inputs, lengths])))\n","            history = EAFP.fit(x=[inputs, lengths], y=labels, epochs=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbcAjVihckb7","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title  P results : Unseen target data loss and  accuracy\n","EAFP.evaluate([inputs_tgt, lengths_tgt], labels_tgt, verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6r4VVtCXXrMn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":266},"cellView":"form","executionInfo":{"status":"ok","timestamp":1593574482480,"user_tz":-330,"elapsed":1210,"user":{"displayName":"Himanshu Tanwar","photoUrl":"","userId":"00569403243595938133"}},"outputId":"c98a4d8a-5fcd-4971-c25e-aec30dd384bc"},"source":["#@title Trainable layers for EAFQ training with fixed embeddings\n","[x.name for x in EAFQ.trainable_variables]"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['DenseAbsExp_0/kernel:0',\n"," 'DenseAbsExp_0/bias:0',\n"," 'DenseAbsExp_1/kernel:0',\n"," 'DenseAbsExp_1/bias:0',\n"," 'DenseAbsExpFinal_1/kernel:0',\n"," 'DenseAbsExpFinal_1/bias:0',\n"," 'DenseAbsExp_0_2/kernel:0',\n"," 'DenseAbsExp_0_2/bias:0',\n"," 'DenseAbsExp_1_2/kernel:0',\n"," 'DenseAbsExp_1_2/bias:0',\n"," 'DenseAbsExpFinal_1_1/kernel:0',\n"," 'DenseAbsExpFinal_1_1/bias:0',\n"," 'DenseTanh/kernel:0',\n"," 'DenseTanh/bias:0']"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"omXqFmLlJ53a","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title train.py : Training F and Q : adversarial : hinge loss\n","#opt.Q_learning_rate = 1e-5\n","EAFQ.compile(optimizer=optimizers.Adam(opt.Q_learning_rate), loss='hinge', metrics=['accuracy'])\n","opt.Q_iterations = 5\n","if TRAIN1:\n","    log.info('Training EAFQ model with src and tgt data with lang_labels and fixed embeddings...')\n","    for epoch in trange(opt.Q_iterations):\n","        batch_no = 0\n","        for (inputs_src, lengths_src, labels_src), (inputs_tgt, lengths_tgt, labels_tgt) in zip(train_src, train_tgt):\n","            log.info(f\"Training on batch no : {batch_no}\"); batch_no += 1\n","            inputs = tf.concat([inputs_src, inputs_tgt], axis=0)\n","            lengths = tf.concat([lengths_src, lengths_tgt], axis=0)\n","            lang_labels_src = tf.broadcast_to([1], shape=labels_src.shape)\n","            lang_labels_tgt = tf.broadcast_to([-1], shape=labels_tgt.shape)\n","            lang_labels = tf.concat([lang_labels_src, lang_labels_tgt], axis=0)\n","            #print(lang_labels)\n","            history = EAFQ.fit(x=[inputs, lengths], y=lang_labels, epochs=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUIKfv2IOZL9","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Q results\n","EAFQ.compile(optimizer=optimizers.Adam(opt.Q_learning_rate), loss='hinge', metrics=['accuracy', keras.metrics.Hinge()])\n","print(EAFQ.predict([inputs_src, lengths_src]), '\\n', EAFQ.evaluate([inputs_src, lengths_src], lang_labels_src, verbose=2), '\\n', EAFQ.predict([inputs_tgt, lengths_tgt]), '\\n', EAFQ.evaluate([inputs_tgt, lengths_tgt], lang_labels_tgt, verbose=2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kxOKkNwjPmd1","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title  P results : Unseen target data loss and  accuracy after F-Q training\n","EAFP.evaluate([inputs_tgt, lengths_tgt], labels_tgt, verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-GvG6a4DfyGp","colab_type":"code","colab":{},"cellView":"form","executionInfo":{"status":"ok","timestamp":1593575532657,"user_tz":-330,"elapsed":1504,"user":{"displayName":"Himanshu Tanwar","photoUrl":"","userId":"00569403243595938133"}}},"source":["#@title Setting the embeddings trainable\n","E.trainable = True\n","EA.trainable = True"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"xNAYQdiucflO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":248},"cellView":"form","executionInfo":{"status":"ok","timestamp":1593575563987,"user_tz":-330,"elapsed":1422,"user":{"displayName":"Himanshu Tanwar","photoUrl":"","userId":"00569403243595938133"}},"outputId":"900f2a2b-9701-4677-b4cc-b9be67d52fe6"},"source":["#@title Trainable layers for EAFP training with trainable embeddings\n","[x.name for x in EAFP.trainable_variables]"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['embeddings:0',\n"," 'DenseAbsExp_0/kernel:0',\n"," 'DenseAbsExp_0/bias:0',\n"," 'DenseAbsExp_1/kernel:0',\n"," 'DenseAbsExp_1/bias:0',\n"," 'DenseAbsExpFinal_1/kernel:0',\n"," 'DenseAbsExpFinal_1/bias:0',\n"," 'DenseAbsExp_0_1/kernel:0',\n"," 'DenseAbsExp_0_1/bias:0',\n"," 'DenseAbsExp_1_1/kernel:0',\n"," 'DenseAbsExp_1_1/bias:0',\n"," 'DenseSoftmax_1/kernel:0',\n"," 'DenseSoftmax_1/bias:0']"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"Dar-cFfxNiby","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":177},"cellView":"form","outputId":"9213020f-3753-46ad-de25-1439152ce18b"},"source":["#@title train.py : Training Embeddings, F and P : sparse categorical\n","EAFP.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","epochs = 5\n","if TRAIN1:\n","    log.info('Training EAFP model with src data with fixed embeddings...')\n","    for epoch in trange(epochs):\n","        batch_no = 0\n","        for (inputs, lengths, labels) in train_src:\n","            log.info(f\" Training on batch no : {batch_no}\"); batch_no += 1\n","            print(scce(labels, EAFP.predict([inputs, lengths])))\n","            history = EAFP.fit(x=[inputs, lengths], y=labels, epochs=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:__main__:Training EAFP model with src data with fixed embeddings...\n","\n","\n","  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[AINFO:__main__:Training on batch no : 0\n"],"name":"stderr"},{"output_type":"stream","text":["tf.Tensor(2.8555007, shape=(), dtype=float32)\n","1563/1563 [==============================] - 2186s 1s/step - loss: 0.6505 - accuracy: 0.6886\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:__main__:Training on batch no : 1\n"],"name":"stderr"},{"output_type":"stream","text":["tf.Tensor(8.91328, shape=(), dtype=float32)\n"," 328/1563 [=====>........................] - ETA: 29:36 - loss: 1.2935 - accuracy: 0.4892"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UujQSmNJdKaK","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Trainable layers for EAFQ training with trainable embeddings\n","[x.name for x in EAFQ.trainable_variables]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b--azhiggzXX","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title train.py : Training Embeddings, F and Q : adversarial : hinge loss\n","#opt.Q_learning_rate = 1e-5\n","EAFQ.compile(optimizer=optimizers.Adam(opt.Q_learning_rate), loss='hinge', metrics=['accuracy'])\n","opt.Q_iterations = 5\n","if TRAIN1:\n","    log.info('Training EAFQ model with src and tgt data with lang_labels and fixed embeddings...')\n","    for epoch in trange(opt.Q_iterations):\n","        batch_no = 0\n","        for (inputs_src, lengths_src, labels_src), (inputs_tgt, lengths_tgt, labels_tgt) in zip(train_src, train_tgt):\n","            log.info(f\" Training on batch no : {batch_no}\"); batch_no += 1\n","            inputs = tf.concat([inputs_src, inputs_tgt], axis=0)\n","            lengths = tf.concat([lengths_src, lengths_tgt], axis=0)\n","            lang_labels_src = tf.broadcast_to([1], shape=labels_src.shape)\n","            lang_labels_tgt = tf.broadcast_to([-1], shape=labels_tgt.shape)\n","            lang_labels = tf.concat([lang_labels_src, lang_labels_tgt], axis=0)\n","            history = EAFQ.fit(x=[inputs, lengths], y=lang_labels, epochs=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9WhKwvRg6nZ","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Trainable layers for LAN training with trainable embeddings\n","[x.name for x in LAN.trainable_variables]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4X6Dw6XyhNhr","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title train.py : Training F and Q : sparse categorical + adversarial : total loss\n","#opt.Q_learning_rate = 1e-5\n","EAFQ.compile(optimizer=optimizers.Adam(opt.learning_rate), loss='hinge', metrics=['accuracy'])\n","LAN.compile(optimizer=optimizers.Adam(opt.learning_rate), loss=total_loss, metrics=['accuracy'])\n","opt.Q_iterations = 5\n","if TRAIN1:\n","    log.info('Training EAFQ model with src and tgt data with lang_labels and fixed embeddings...')\n","    for epoch in trange(opt.Q_iterations):\n","        batch_no = 0\n","        for (inputs_src, lengths_src, labels_src), (inputs_tgt, lengths_tgt, labels_tgt) in zip(train_src, train_tgt):\n","            log.info(f\" Training on batch no : {batch_no}\"); batch_no += 1\n","            inputs = tf.concat([inputs_src, inputs_tgt], axis=0)\n","            lengths = tf.concat([lengths_src, lengths_tgt], axis=0)\n","            labels = tf.concat([labels_src, labels_tgt], axis=0)\n","            lang_labels_src = tf.broadcast_to([1], shape=labels_src.shape)\n","            lang_labels_tgt = tf.broadcast_to([-1], shape=labels_tgt.shape)\n","            lang_labels = tf.concat([lang_labels_src, lang_labels_tgt], axis=0)\n","            history_EAFQ = EAFQ.fit(x=[inputs, lengths], y=lang_labels, epochs=1)\n","            history_LAN = LAN.fit(x=[inputs_src, lengths_src], y=[labels_src, lang_labels_src], epochs=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-atO5MZiS-X","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Sentiment classifier results on unseen target data\n","EAFP.evaluate(x=[inputs_tgt, lengths_tgt], y=labels_tgt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lv4HTDiyla0A","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title Overall LAN results on unseen target data\n","LAN.evaluate(x=[inputs_tgt, lengths_tgt], y=[labels_tgt, lang_labels_tgt])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsWwzDmMqpD3","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-Y7LCFzaBEc","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title train.py (original thesis model - incomplete) - DO NOT RUN\n","\n","#!/usr/bin/env ipython\n","# foo = open('train.py', 'r'); foo.readline(); exec(foo.read()); foo.close()\n","#import torch\n","#import torch.nn as nn\n","#import torch.nn.functional as functional\n","#import torch.optim as optim\n","#from torch.utils.data import DataLoader\n","#from torchnet.meter import ConfusionMeter\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras import optimizers, losses\n","import json\n","\n","import os, random, sys, logging, argparse\n","from tqdm import tqdm\n","from pathlib import Path\n","os.chdir(os.path.dirname(__file__))\n","\n","#from options import *\n","#from data import *\n","#from vocab import *\n","#from utils import *\n","#from models import *\n","#from train_data import *\n","\n","#tf.logging.set_verbosity(tf.logging.INFO)\n","#tf.logging.set_verbosity(True)\n","\n","#random.seed(opt.random_seed)\n","#torch.manual_seed(opt.random_seed)\n","\n","# save logs\n","if not os.path.exists(opt.model_save_file): os.makedirs(opt.model_save_file)\n","logging.basicConfig(stream=sys.stderr, level=logging.DEBUG if opt.debug else logging.INFO)\n","log = logging.getLogger(__name__)\n","fh = logging.FileHandler(os.path.join(opt.model_save_file, 'log.txt'))\n","log.addHandler(fh)\n","\n","#vars = ['vocab', 'train_src', 'dev_src', 'test_src', 'train_tgt', 'dev_tgt', 'test_tgt', 'train_src_Q', 'train_tgt_Q', 'train_src_Q_iter', 'train_tgt_Q_iter', 'length']\n","#for var in vars:\n","#    if var not in locals() and var not in globals(): print(var, 'not imported '); exit()\n","#    print(var, 'imported')\n","\n","\n","#def train(opt):\n","if __name__ == \"__main__\" and not TRAIN1:\n","    \"\"\"Train Process:\n","Require => labeled SOURCE corpus Xsrc; unlabeled TARGET corpus Xtgt; Hyperpamameter  > 0, k  N, c > 0; Lp(y, y) crossentropy loss.\n","=> Main iteration\n","repeat\n","    => Q iterations\n","    for qiter = 1 to k do\n","        Sample unlabeled batch x_src ~ X_src\n","        Sample unlabeled batch x_tgt ~ X_tgt\n","        f_src = F (x_src)\n","        f_tgt = F (x_tgt) . feature vectors\n","        lossq = -Q(f_src) + Q(f_tgt)\n","        Update Q parameters to minimize lossq\n","        ClipWeights(Q, -c, c)\n","    \n","    => F&P iteration\n","    Sample labeled batch (x_src, y_src) ~ Xsrc\n","    Sample unlabeled batch xtgt ~ Xtgt\n","    f_src = F (x_src)\n","    f_tgt = F (x_tgt)\n","    loss = Lp(P(f_src); y_src) +  * (Q(f_src) - Q(f_tgt))\n","    Update F , P parameters to minimize loss\n","until convergence\n","    \"\"\"\n","    DEBUG = lambda x : print('__DEBUG__ : ', x)\n","    # data\n","    if not opt.notebook: vocab, train_src, dev_src, test_src, train_tgt, dev_tgt, test_tgt, train_src_Q, train_tgt_Q, train_src_Q_iter, train_tgt_Q_iter, length = get_train_data(opt)\n","\n","    # models\n","    log.info('Initializing models...')\n","    if opt.model.lower() == 'dan': F = DAN_Feature_Extractor(vocab, opt.F_layers, opt.hidden_size, opt.dropout, opt.F_bn)\n","    elif opt.model.lower() == 'lstm': F = LSTM_Feature_Extractor(vocab, opt.F_layers, opt.hidden_size, opt.dropout, opt.bdrnn, opt.attn)\n","    elif opt.model.lower() == 'cnn': F = CNN_Feature_Extractor(vocab, opt.F_layers, opt.hidden_size, opt.kernel_num, opt.kernel_sizes, opt.dropout)\n","    else: raise Exception('Unknown model')\n","\n","    P = Sentiment_Classifier(opt.P_layers, opt.hidden_size, opt.num_labels, opt.dropout, opt.P_bn)\n","    Q = Language_Detector(opt.Q_layers, opt.hidden_size, opt.dropout, opt.Q_bn)\n","    log.info('Done...')\n","\n","    optimizer_FP = Optimizer_FP(models=[F, P, Q], lr=opt.learning_rate, clip_lim=opt.clip_lim_FP)\n","    if not opt.clip_Q: optimizer_Q = optimizers.Adam(lr=opt.Q_learning_rate)\n","    else: optimizer_Q = optimizers.Adam(lr=opt.Q_learning_rate, clipvalue=opt.clipvalue)\n","    \n","    F.fcnet.compile(optimizer=optimizer_FP)\n","    P.net.compile(optimizer=optimizer_FP)\n","    Q.compile(optimizer=optimizer_Q)\n","\n","    best_acc = 0.0\n","    # train tgt iterator\n","    train_tgt_iter = iter(train_tgt)\n","    log.info('Main Iteration begin...')\n","    \"\"\" Main iterations \"\"\"\n","    for epoch in trange(opt.epochs):\n","        F.unfreeze()\n","        P.unfreeze()\n","        Q.unfreeze()\n","        F.freeze_emb_layer()\n","        \n","        # for training accuracy\n","        correct, total = 0, 0\n","        sum_src_q, sum_tgt_q = (0, 0.0), (0, 0.0)    # qiter number, loss_q\n","        grad_norm_p, grad_norm_q = (0, 0.0), (0, 0.0)\n","        \n","        # train src iterator\n","        train_src_iter = iter(train_src)\n","        log.info('Q iteration begin...')\n","        for i, (inputs_src, lengths_src, labels_src) in tqdm(enumerate(train_src_iter), total=(length['train_src'] + opt.batch_size - 1)//opt.batch_size):\n","            \"\"\" sample batches: labeled (xsrc, ysrc) in Xsrc \"\"\"\n","            \"\"\" sample unlabeled xtgt in Xtgt \"\"\"\n","            try:\n","                inputs_tgt, _, _ = next(train_tgt_iter)  # tgt labels not used\n","            except:\n","                # check if tgt data is exhausted\n","                train_tgt_iter = iter(train_tgt)\n","                inputs_tgt, _, _ = next(train_tgt_iter)\n","            \n","            \"\"\" Q iterations: \"\"\"\n","            q_critic = 1 #opt.q_critic\n","            #if q_critic>0 and ((epoch==0 and i<=25) or (i%500==0)): q_critic = 10\n","            #F.freeze()\n","            #P.freeze()\n","            #Q.unfreeze()\n","            #F.freeze_emb_layer()\n","            #Q.clip_weights()\n","\n","            for qiter in range(q_critic):\n","                \"\"\" sample unlabeled batches: xsrc in Xsrc, xtgt in Xtgt \"\"\"\n","                # get a minibatch of data\n","                try:\n","                    # labels are not used\n","                    inputs_src_Q, lengths_src_Q, _ = next(train_src_Q_iter)\n","                except StopIteration:\n","                    # check if dataloader is exhausted\n","                    train_src_Q_iter = iter(train_src_Q)\n","                    inputs_src_Q, lengths_src_Q, _ = next(train_src_Q_iter)\n","                try:\n","                    inputs_tgt_Q, lengths_tgt_Q, _ = next(train_tgt_Q_iter)\n","                except StopIteration:\n","                    train_tgt_Q_iter = iter(train_tgt_Q)\n","                    inputs_tgt_Q, lengths_tgt_Q, _ = next(train_tgt_Q_iter)\n","                \n","                DEBUG(\"\"\" extract features : f_src, f_tgt = F(x_src), F(x_tgt) \"\"\")\n","                features_src = F(inputs_src_Q, lengths_src_Q)\n","                features_tgt = F(inputs_tgt_Q, lengths_tgt_Q)\n","                \n","                \"\"\" calculate loss_q : loss_q = -Q(f_src) + Q(f_tgt) \"\"\"\n","                DEBUG(\"\"\" update Q to minimise loss_q \"\"\")\n","                l_src_ad = Q.train_step(features_src, 'src', loss='scce')['loss']\n","                l_tgt_ad = Q.train_step(features_tgt, 'tgt', loss='scce')['loss']\n","                # summed Q losses\n","                #sum_src_q = (sum_src_q[0] + 1, sum_src_q[1] + l_src_ad)\n","                #sum_tgt_q = (sum_tgt_q[0] + 1, sum_tgt_q[1] + l_tgt_ad)\n","\n","                DEBUG(\"\"\" clip Q weights \"\"\")\n","                #Q.clip_weights()\n","            \n","            log.info('Q iteration done.')\n","\n","            \"\"\" F&P iteration \"\"\"\n","            #F.unfreeze()\n","            #P.unfreeze()\n","            #Q.freeze()\n","            #if opt.fix_emb: F.freeze_emb_layer()\n","            #elif epoch>3: F.unfreeze_emb_layer()\n","            #F.unfreeze_emb_layer()\n","\n","            \"\"\" extract features : f_src, f_tgt = F(x_src), F(x_tgt) \"\"\"\n","            DEBUG(\"\"\" calculate loss : loss = Lp(P(f_src); y_src) +  * (Q(f_src) - Q(f_tgt)) \"\"\")\n","            #metrices = optimizer_FP.call(inputs_src, inputs_tgt, labels_src, labels_tgt=None, _lambda=opt._lambda, supervised=False)\n","            #pred = argmax32(o_src_sent)\n","            #total += len(labels_src)\n","            #correct += np.sum(pred == labels_src)\n","\n","        #log.info('\\n\\nl_src_ad = \\n' + str(l_src_ad))\n","        #log.info('\\n\\nl_tgt_ad = \\n' + str(l_tgt_ad))\n","        #log.info(f'\\n\\n result :\\n' + str(metrices))\n","\n","    log.info('\\nMain iteration done.')\n","    log.info(f' (Q(features_src) < Q(features_tgt)) : {np.sum(Q(features_src) < Q(features_tgt))}')\n","    log.info(f' (Q(features_src) > Q(features_tgt)) : {np.sum(Q(features_src) > Q(features_tgt))}')\n","    log.info(f' Q precision in differentiating src-tgt : {np.sum(Q(features_src) > Q(features_tgt)) / (np.sum(Q(features_src) < Q(features_tgt)) + np.sum(Q(features_src) > Q(features_tgt)))}')\n","    log.info(f' Q accuracy : unknown')\n","    #log.info(f'\\n\\n RESULT :\\n' + str(metrices))\n","\n","# train.py\n","#if __name__ == \"__main__\":\n","#    train(opt)"],"execution_count":null,"outputs":[]}]}
